---
systemd:
  units:
    - name: docker.service
      enable: true

    - name: oem-cloudinit.service
      enable: false
      mask: true

    - name: coreos-metadata-sshkeys@.service
      enable: false
      mask: true

    - name: locksmithd.service
      enable: false
      mask: true

    - name: containerd.service
      enable: true

    - name: update-engine.service
      enable: false
      mask: true

    - name: update-engine-stub.service
      enable: false
      mask: true

    - name: tpmd.service
      enable: false
      mask: true

    - name: tcsd.service
      enable: false
      mask: true

    - name: cni-gc.service
      enable: true
      contents: |
        [Unit]
        Before=network-online.target
        RefuseManualStart=true

        [Service]
        Type=oneshot
        RemainAfterExit=true
        ExecStartPre=-/bin/rm -Rfv /var/lib/cni
        ExecStart=/bin/mkdir -pv /var/lib/cni/networks
        Restart=no

        [Install]
        RequiredBy=multi-user.target

    - name: docker-gc.timer
      enable: true
      contents: |
        [Timer]
        OnBootSec=2min
        OnUnitActiveSec=5min
        [Install]
        WantedBy=timers.target

    - name: docker-gc.service
      enable: true
      contents: |
        [Unit]
        Requires=docker.service
        After=docker.service

        [Service]
        Type=oneshot
        ExecStart=-/bin/bash -c '/usr/bin/docker rm $(/usr/bin/docker ps -aq)'
        Restart=no

        [Install]
        RequiredBy=multi-user.target

    - name: rkt-api.service
      enable: true
      contents: |
        [Service]
        LimitNOFILE=65826
        ExecStart=/usr/bin/rkt api-service
        Restart=always
        RestartSec=2

        [Install]
        RequiredBy=multi-user.target

    - name: rkt-metadata.service
      enable: true

    - name: enjoliver-agent.service
      enable: true
      contents: |
        [Service]
        ExecStart=/usr/bin/enjoliver-agent --control-plane
        EnvironmentFile=/etc/metadata.env
        Restart=always
        RestartSec=30

        [Install]
        RequiredBy=multi-user.target

    - name: systemd-timesyncd.service
      enable: true

    - name: ntpd.service
      enable: false

    - name: etcd-operator@vault.service
      enable: true
      contents: |
        [Unit]
        Requires=network-online.target
        After=network-online.target

        [Service]
        EnvironmentFile=/etc/etcd-%i
        Environment=ETCD_ENV_FILE=/etc/etcd-%i.env
        ExecStartPre=/opt/bin/ping-one-cp
        ExecStart=/opt/bin/etcd-operator
        RestartSec=15s
        Restart=on-failure

        [Install]
        RequiredBy=multi-user.target

    - name: etcd-operator@kubernetes.service
      enable: true
      contents: |
        [Unit]
        Requires=network-online.target
        After=network-online.target

        [Service]
        EnvironmentFile=/etc/etcd-%i
        Environment=ETCD_ENV_FILE=/etc/etcd-%i.env
        ExecStartPre=/opt/bin/ping-one-cp
        ExecStart=/opt/bin/etcd-operator
        RestartSec=15s
        Restart=on-failure

        [Install]
        RequiredBy=multi-user.target

    - name: etcd-operator@fleet.service
      enable: true
      contents: |
        [Unit]
        Requires=network-online.target
        After=network-online.target

        [Service]
        EnvironmentFile=/etc/etcd-%i
        Environment=ETCD_ENV_FILE=/etc/etcd-%i.env
        ExecStartPre=/opt/bin/ping-one-cp
        ExecStart=/opt/bin/etcd-operator
        RestartSec=15s
        Restart=on-failure

        [Install]
        RequiredBy=multi-user.target

    - name: etcd.service
      mask: true
      enable: false

    - name: etcd2.service
      mask: true
      enable: false

    - name: etcd3@vault.service
      enable: true
      contents: |
        [Unit]
        Conflicts=etcd.service etcd2.service
        After=etcd-operator@%i.service

        [Service]
        Type=notify
        EnvironmentFile=/etc/etcd-%i.env
        ExecStart=/usr/bin/etcd --auto-tls --peer-auto-tls
        RestartSec=15s
        Restart=on-failure
        LimitNOFILE=65826

        [Install]
        WantedBy=multi-user.target

    - name: vault.service
      enable: true
      contents: |
        [Unit]
        After=etcd3@vault.service

        [Service]
        ExecStartPre=/usr/bin/etcdctl --no-sync --endpoints http://127.0.0.1:{{.vault_etcd_client_port}} ls
        ExecStartPre=/opt/bin/vault-config
        ExecStart=/usr/bin/vault server -config=/etc/vault.d/
        Restart=always
        RestartSec=5

        [Install]
        WantedBy=multi-user.target

    - name: vault-init.service
      enable: true
      contents: |
        [Unit]
        After=vault.service

        [Service]
        ExecStartPre=/usr/bin/vault version
        ExecStart=/opt/bin/vault-init
        RestartSec=15
        Restart=on-failure

        [Install]
        WantedBy=multi-user.target

    - name: vault-token-server@vault.service
      enable: true
      contents: |
        [Service]
        ExecStartPre=/bin/mkdir -pv /etc/vault.d/%i
        ExecStart=/opt/bin/vault-token %i server
        RestartSec={{.vault_polling_sec}}
        Restart=on-failure

        [Install]
        WantedBy=multi-user.target

    - name: vault-token-peer@etcd-kubernetes.service
      enable: true
      contents: |
        [Service]
        ExecStartPre=/bin/mkdir -pv /etc/vault.d/%i
        ExecStart=/opt/bin/vault-token %i peer
        RestartSec={{.vault_polling_sec}}
        Restart=on-failure

        [Install]
        WantedBy=multi-user.target

    - name: vault-token-client@etcd-kubernetes.service
      enable: true
      contents: |
        [Service]
        ExecStartPre=/bin/mkdir -pv /etc/vault.d/%i
        ExecStart=/opt/bin/vault-token %i client
        RestartSec={{.vault_polling_sec}}
        Restart=on-failure

        [Install]
        WantedBy=multi-user.target

    - name: vault-token-kubelet@kubernetes.service
      enable: true
      contents: |
        [Service]
        ExecStartPre=/bin/mkdir -pv /etc/vault.d/%i
        ExecStart=/opt/bin/vault-token %i kubelet
        RestartSec={{.vault_polling_sec}}
        Restart=on-failure

        [Install]
        WantedBy=multi-user.target

    - name: vault-token-kube-apiserver@kubernetes.service
      enable: true
      contents: |
        [Service]
        ExecStartPre=/bin/mkdir -pv /etc/vault.d/%i
        ExecStart=/opt/bin/vault-token %i kube-apiserver
        RestartSec={{.vault_polling_sec}}
        Restart=on-failure

        [Install]
        WantedBy=multi-user.target

    - name: vault-token-peer@etcd-fleet.service
      enable: true
      contents: |
        [Service]
        ExecStartPre=/bin/mkdir -pv /etc/vault.d/%i
        ExecStart=/opt/bin/vault-token %i peer
        RestartSec={{.vault_polling_sec}}
        Restart=on-failure

        [Install]
        WantedBy=multi-user.target

    - name: vault-token-client@etcd-fleet.service
      enable: true
      contents: |
        [Service]
        ExecStartPre=/bin/mkdir -pv /etc/vault.d/%i
        ExecStart=/opt/bin/vault-token %i client
        RestartSec={{.vault_polling_sec}}
        Restart=on-failure

        [Install]
        WantedBy=multi-user.target

    - name: vault-pki-issue-server@vault.service
      enable: true
      contents: |
        [Service]
        EnvironmentFile=/etc/vault.d/%i/server.token
        ExecStart=/opt/bin/vault-pki-issue %i server
        RestartSec={{.vault_polling_sec}}
        Restart=on-failure

        [Install]
        WantedBy=multi-user.target

    - name: vault-pki-issue-peer@etcd-kubernetes.service
      enable: true
      contents: |
        [Service]
        EnvironmentFile=/etc/vault.d/%i/peer.token
        ExecStart=/opt/bin/vault-pki-issue %i peer
        RestartSec={{.vault_polling_sec}}
        Restart=on-failure

        [Install]
        WantedBy=multi-user.target

    - name: vault-pki-issue-client@etcd-kubernetes.service
      enable: true
      contents: |
        [Service]
        EnvironmentFile=/etc/vault.d/%i/client.token
        ExecStart=/opt/bin/vault-pki-issue %i client
        RestartSec={{.vault_polling_sec}}
        Restart=on-failure

        [Install]
        WantedBy=multi-user.target

    - name: vault-pki-issue-kubelet@kubernetes.service
      enable: true
      contents: |
        [Service]
        EnvironmentFile=/etc/vault.d/%i/kubelet.token
        ExecStart=/opt/bin/vault-pki-issue %i kubelet
        RestartSec={{.vault_polling_sec}}
        Restart=on-failure

        [Install]
        WantedBy=multi-user.target

    - name: vault-pki-issue-kube-apiserver@kubernetes.service
      enable: true
      contents: |
        [Service]
        EnvironmentFile=/etc/vault.d/%i/kube-apiserver.token
        ExecStart=/opt/bin/vault-pki-issue %i kube-apiserver
        RestartSec={{.vault_polling_sec}}
        Restart=on-failure

        [Install]
        WantedBy=multi-user.target

    - name: vault-secret-service-accounts@kubernetes.service
      enable: true
      contents: |
        [Service]
        EnvironmentFile=/etc/vault.d/%i/kube-apiserver.token
        ExecStart=/opt/bin/vault-secret %i service-accounts
        RestartSec={{.vault_polling_sec}}
        Restart=on-failure

        [Install]
        WantedBy=multi-user.target

    - name: vault-pki-issue-peer@etcd-fleet.service
      enable: true
      contents: |
        [Service]
        EnvironmentFile=/etc/vault.d/%i/peer.token
        ExecStart=/opt/bin/vault-pki-issue %i peer
        RestartSec={{.vault_polling_sec}}
        Restart=on-failure

        [Install]
        WantedBy=multi-user.target

    - name: vault-pki-issue-client@etcd-fleet.service
      enable: true
      contents: |
        [Service]
        EnvironmentFile=/etc/vault.d/%i/client.token
        ExecStart=/opt/bin/vault-pki-issue %i client
        RestartSec={{.vault_polling_sec}}
        Restart=on-failure

        [Install]
        WantedBy=multi-user.target

    - name: etcd3@kubernetes.service
      enable: true
      contents: |
        [Unit]
        Conflicts=etcd.service etcd2.service
        After=etcd-operator@%i.service

        [Service]
        Type=notify
        EnvironmentFile=/etc/etcd-%i.env
        ExecStartPre=/bin/ls -l \
          /etc/vault.d/etcd-%i/client.certificate \
          /etc/vault.d/etcd-%i/client.private_key \
          /etc/vault.d/etcd-%i/client.issuing_ca \
          /etc/vault.d/etcd-%i/peer.certificate \
          /etc/vault.d/etcd-%i/peer.private_key \
          /etc/vault.d/etcd-%i/peer.issuing_ca
        ExecStart=/usr/bin/etcd \
          --cert-file /etc/vault.d/etcd-%i/client.certificate \
          --key-file /etc/vault.d/etcd-%i/client.private_key \
          --trusted-ca-file /etc/vault.d/etcd-%i/client.issuing_ca \
          --client-cert-auth \
          \
          --peer-cert-file /etc/vault.d/etcd-%i/peer.certificate \
          --peer-key-file /etc/vault.d/etcd-%i/peer.private_key \
          --peer-trusted-ca-file /etc/vault.d/etcd-%i/peer.issuing_ca \
          --peer-client-cert-auth \
          --auto-compaction-retention=12
        RestartSec=20s
        Restart=on-failure
        LimitNOFILE=65826

        [Install]
        WantedBy=multi-user.target

    - name: etcd3@fleet.service
      enable: true
      contents: |
        [Unit]
        Conflicts=etcd.service etcd2.service
        After=etcd-operator@%i.service

        [Service]
        Type=notify
        EnvironmentFile=/etc/etcd-%i.env
        ExecStartPre=/bin/ls -l \
          /etc/vault.d/etcd-%i/client.certificate \
          /etc/vault.d/etcd-%i/client.private_key \
          /etc/vault.d/etcd-%i/client.issuing_ca \
          /etc/vault.d/etcd-%i/peer.certificate \
          /etc/vault.d/etcd-%i/peer.private_key \
          /etc/vault.d/etcd-%i/peer.issuing_ca
        ExecStart=/usr/bin/etcd \
          --cert-file /etc/vault.d/etcd-%i/client.certificate \
          --key-file /etc/vault.d/etcd-%i/client.private_key \
          --trusted-ca-file /etc/vault.d/etcd-%i/client.issuing_ca \
          --client-cert-auth \
          \
          --peer-cert-file /etc/vault.d/etcd-%i/peer.certificate \
          --peer-key-file /etc/vault.d/etcd-%i/peer.private_key \
          --peer-trusted-ca-file /etc/vault.d/etcd-%i/peer.issuing_ca \
          --peer-client-cert-auth
        RestartSec=15s
        Restart=on-failure
        LimitNOFILE=65826

        [Install]
        WantedBy=multi-user.target

    - name: kube-apiserver.service
      enable: true
      contents: |
        [Unit]
        After=etcd3@kubernetes.service

        [Service]
        Environment=ETCDCTL_API=3
        ExecStartPre=/bin/ls -l \
          /etc/vault.d/kubernetes/kube-apiserver.certificate \
          /etc/vault.d/kubernetes/kube-apiserver.private_key \
          /etc/vault.d/kubernetes/kube-apiserver.issuing_ca \
          /etc/vault.d/kubernetes/kubelet.certificate \
          /etc/vault.d/kubernetes/kubelet.private_key \
          /etc/vault.d/kubernetes/kubelet.issuing_ca \
          /etc/vault.d/kubernetes/service-accounts.key
        ExecStartPre=/usr/bin/etcdctl --endpoints http://127.0.0.1:{{.kubernetes_etcd_client_port}} endpoint status
        ExecStart=/usr/bin/hyperkube apiserver \
          --apiserver-count={{.kubernetes_apiserver_count}} \
          --advertise-address={{.network.ip_or_fqdn}} \
          --insecure-bind-address={{.kubernetes_apiserver_insecure_bind_address}} \
          --insecure-port={{.kubernetes_apiserver_insecure_port}} \
          --allow-privileged=true \
          --service-cluster-ip-range={{.kubernetes_service_cluster_ip_range}} \
          --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \
          --runtime-config=batch/v2alpha1 \
          --tls-ca-file=/etc/vault.d/kubernetes/kube-apiserver.issuing_ca \
          --tls-cert-file=/etc/vault.d/kubernetes/kube-apiserver.certificate \
          --tls-private-key-file=/etc/vault.d/kubernetes/kube-apiserver.private_key \
          --client-ca-file=/etc/vault.d/kubernetes/kube-apiserver.issuing_ca \
          --kubelet-certificate-authority=/etc/vault.d/kubernetes/kubelet.issuing_ca \
          --kubelet-client-certificate=/etc/vault.d/kubernetes/kubelet.certificate \
          --kubelet-client-key=/etc/vault.d/kubernetes/kubelet.private_key \
          --kubelet-https=true \
          --kubelet-preferred-address-types=InternalIP,LegacyHostIP,ExternalDNS,InternalDNS,Hostname \
          --service-account-key-file=/etc/vault.d/kubernetes/service-accounts.key \
          --service-account-lookup=true \
          --authorization-mode=RBAC \
          --anonymous-auth=false \
          --etcd-servers=http://127.0.0.1:{{.kubernetes_etcd_client_port}}
        Restart=always
        RestartSec=5s

        [Install]
        WantedBy=multi-user.target

    - name: kubelet.service
      enable: true
      contents: |
        [Unit]
        After=rkt-api.service
        Requires=rkt-api.service
        After=kube-apiserver.service

        [Service]
        Environment=PATH=/opt/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin
        ExecStartPre=/bin/ls -l \
          /etc/vault.d/kubernetes/kubelet.certificate \
          /etc/vault.d/kubernetes/kubelet.private_key \
          /etc/vault.d/kubernetes/kubelet.issuing_ca

        ExecStart=/usr/bin/hyperkube kubelet \
          --hairpin-mode=none \
          --non-masquerade-cidr={{.network.subnet}} \
          --network-plugin=cni \
          --cni-bin-dir=/usr/local/cni/bin/ \
          --cni-conf-dir=/etc/rkt/net.d \
          --hostname-override={{.kubernetes_node_name}} \
          --node-ip={{.kubernetes_node_ip}} \
          --register-schedulable=false \
          --allow-privileged=true \
          --enable-custom-metrics \
          --kubeconfig=/var/lib/kubelet/kubeconfig \
          --node-labels=control-plane=true,diskProfile={{.disk_profile}} \
          --cloud-provider="" \
          --container-runtime=docker \
          --v=2 \
          --tls-cert-file=/etc/vault.d/kubernetes/kubelet.certificate \
          --tls-private-key-file=/etc/vault.d/kubernetes/kubelet.private_key \
          --client-ca-file=/etc/vault.d/kubernetes/kubelet.issuing_ca \
          --healthz-port {{.kubelet_healthz_port}}
        Restart=always
        RestartSec=5s

        [Install]
        WantedBy=multi-user.target

    - name: kube-control-plane.service
      enable: true
      contents: |
        [Unit]
        After=kube-apiserver.service

        [Service]
        ExecStartPre=/usr/bin/curl -f http://127.0.0.1:{{.kubernetes_apiserver_insecure_port}}/healthz
        ExecStart=/opt/bin/create-control-plane
        Restart=on-failure
        RestartSec=5

        [Install]
        WantedBy=multi-user.target

    - name: fleet.service
      enable: true
      dropins:
        - name: 10-cluster.conf
          contents: |
            [Unit]
            After=etcd3@fleet.service
            After=rkt-metadata.service

            [Service]
            EnvironmentFile=/etc/metadata.env
            ExecStartPre=/usr/bin/etcdctl --no-sync --endpoint http://127.0.0.1:{{.fleet_etcd_client_port}} ls
            Restart=always
            RestartSec=10

    - name: lifecycle-ready.service
      enable: true
      contents: |
        [Unit]
        After=etcd3@kubernetes.service
        After=kubelet.service

        [Service]
        Type=oneshot
        ExecStart=/opt/bin/lifecycle-ready
        RemainAfterExit=yes

        [Install]
        WantedBy=multi-user.target

    - name: lifecycle-update.service
      enable: true
      contents: |
        [Unit]
        After=etcd3@vault.service
        After=etcd3@fleet.service
        After=etcd3@kubernetes.service
        After=kubelet.service
        After=lifecycle-ready.service

        [Service]
        EnvironmentFile=/etc/metadata.env
        ExecStart=/opt/bin/lifecycle-update
        Restart=always
        RestartSec={{.lifecycle_update_polling_sec}}

        [Install]
        WantedBy=multi-user.target

    - name: rkt-gc.service
      enable: true
      dropins:
        - name: 10-rkt.conf
          contents: |
            [Service]
            # drop-in bugs if don't create an empty entry
            ExecStart=
            # rkt doesn't remove used images
            ExecStartPre=-/bin/bash -c '/usr/bin/rkt image list --no-legend | awk \'$8 ~ /(day|week|month)s?/ { system("/usr/bin/rkt image rm " $1) }\''
            # safety for the rkt lock
            ExecStartPre=-/bin/sleep 1
            ExecStartPre=-/usr/bin/rkt image gc --grace-period=12h
            # because the run-prepared by kubernetes can be gc
            ExecStart=/usr/bin/rkt gc --grace-period=3m
            [Install]
            RequiredBy=multi-user.target

    - name: rkt-gc.service
      enable: true
      contents: |
        [Service]
        ExecStartPre=-/bin/bash -c '/usr/bin/rkt image list --no-legend | awk \'$8 ~ /(day|week|month)s?/ { system("/usr/bin/rkt image rm " $1) }\''
        ExecStartPre=-/bin/sleep 1
        ExecStartPre=-/usr/bin/rkt image gc --grace-period=12h
        ExecStart=/usr/bin/rkt gc --grace-period=3m
        [Install]
        RequiredBy=multi-user.target

    - name: rkt-gc.timer
      enable: true
      contents: |
        [Timer]
        OnActiveSec=4min
        OnUnitActiveSec=10min
        [Install]
        WantedBy=timers.target

storage:
  files:
    - path: /etc/kubernetes/manifests/vault.yaml
      filesystem: root
      contents:
        inline: |
          kind: Service
          apiVersion: v1
          metadata:
            name: vault
            namespace: kube-system
            labels:
              app: vault
          spec:
            ports:
            - port: {{.vault_port}}
              protocol: TCP
            clusterIP: "None"
          ---
          kind: Endpoints
          apiVersion: v1
          metadata:
            name: vault
            namespace: kube-system
            labels:
              app: vault
          subsets:
            - addresses:
              {{ range $element := .etcd_member_kubernetes_control_plane_ip }}
                - ip: {{$element}}
              {{end}}
              ports:
                - port: {{.vault_port}}

    - path: /etc/kubernetes/manifests/etcd-vault.yaml
      filesystem: root
      contents:
        inline: |
          kind: Service
          apiVersion: v1
          metadata:
            name: etcd-vault
            namespace: kube-system
            labels:
              db: etcd
              etcd: vault
          spec:
            ports:
            - port: {{.vault_etcd_client_port}}
              protocol: TCP
            clusterIP: "None"
          ---
          kind: Endpoints
          apiVersion: v1
          metadata:
            name: etcd-vault
            namespace: kube-system
            labels:
              db: etcd
              etcd: vault
          subsets:
            - addresses:
              {{ range $element := .etcd_member_kubernetes_control_plane_ip }}
                - ip: {{$element}}
              {{end}}
              ports:
                - port: {{.vault_etcd_client_port}}

    - path: /etc/kubernetes/manifests/etcd-kubernetes.yaml
      filesystem: root
      contents:
        inline: |
          kind: Service
          apiVersion: v1
          metadata:
            name: etcd-kubernetes
            namespace: kube-system
            labels:
              db: etcd
              etcd: kubernetes
          spec:
            ports:
            - port: {{.kubernetes_etcd_client_port}}
              protocol: TCP
            clusterIP: "None"
          ---
          kind: Endpoints
          apiVersion: v1
          metadata:
            name: etcd-kubernetes
            namespace: kube-system
            labels:
              db: etcd
              etcd: kubernetes
          subsets:
            - addresses:
              {{ range $element := .etcd_member_kubernetes_control_plane_ip }}
                - ip: {{$element}}
              {{end}}
              ports:
                - port: {{.kubernetes_etcd_client_port}}

    - path: /etc/kubernetes/manifests/etcd-fleet.yaml
      filesystem: root
      contents:
        inline: |
          kind: Service
          apiVersion: v1
          metadata:
            name: etcd-fleet
            namespace: kube-system
            labels:
              db: etcd
              etcd: fleet
          spec:
            ports:
            - port: {{.fleet_etcd_client_port}}
              protocol: TCP
            clusterIP: "None"
          ---
          kind: Endpoints
          apiVersion: v1
          metadata:
            name: etcd-fleet
            namespace: kube-system
            labels:
              db: etcd
              etcd: fleet
          subsets:
            - addresses:
              {{ range $element := .etcd_member_kubernetes_control_plane_ip }}
                - ip: {{$element}}
              {{end}}
              ports:
                - port: {{.fleet_etcd_client_port}}

    - path: /etc/kubernetes/manifests/kube-controller-manager.yaml
      filesystem: root
      contents:
        inline: |
          kind: Service
          apiVersion: v1
          metadata:
            name: kube-controller-manager
            namespace: kube-system
          spec:
            selector:
              app: kube-controller-manager
            ports:
            - port: 10252
              protocol: TCP
            clusterIP: "None"
          ---
          apiVersion: v1
          kind: ServiceAccount
          metadata:
            name: kube-controller-manager
            namespace: kube-system
          automountServiceAccountToken: false
          ---
          apiVersion: v1
          kind: Pod
          metadata:
            labels:
              app: kube-controller-manager
            name: kube-controller-manager-{{.kubernetes_node_name}}
            namespace: kube-system
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
              prometheus.io/scrape: "true"
              prometheus.io/path: "/metrics"
              prometheus.io/port: "10252"
          spec:
            serviceAccountName: kube-controller-manager
            automountServiceAccountToken: false
            nodeName: "{{.kubernetes_node_name}}"
            hostNetwork: true
            containers:
            - name: kube-controller-manager
              image: {{.hyperkube_image_url}}
              imagePullPolicy: IfNotPresent
              command:
              - /hyperkube
              - controller-manager
              - --master=http://127.0.0.1:{{.kubernetes_apiserver_insecure_port}}
              - --leader-elect=true
              - --service-account-private-key-file=/etc/secrets/service-accounts.key
              - --cluster-signing-cert-file=/etc/secrets/kube-apiserver.certificate
              - --cluster-signing-key-file=/etc/secrets/kube-apiserver.private_key
              - --root-ca-file=/etc/secrets/kube-apiserver.issuing_ca
              resources:
                requests:
                  cpu: 200m
              livenessProbe:
                httpGet:
                  host: 127.0.0.1
                  path: /healthz
                  port: 10252
                initialDelaySeconds: 15
                timeoutSeconds: 15
              readinessProbe:
                failureThreshold: 3
                httpGet:
                  host: 127.0.0.1
                  path: /healthz
                  port: 10252
                initialDelaySeconds: 5
                timeoutSeconds: 10
                periodSeconds: 1
              volumeMounts:
              - name: secrets
                mountPath: /etc/secrets
                readOnly: true
            volumes:
            - name: secrets
              hostPath:
                path: /etc/vault.d/kubernetes

    - path: /etc/kubernetes/manifests/kube-scheduler.yaml
      filesystem: root
      contents:
        inline: |
          kind: Service
          apiVersion: v1
          metadata:
            name: kube-scheduler
            namespace: kube-system
          spec:
            selector:
              app: kube-scheduler
            ports:
            - port: 10251
              protocol: TCP
            clusterIP: "None"
          ---
          apiVersion: extensions/v1beta1
          kind: DaemonSet
          metadata:
            name: kube-scheduler
            namespace: kube-system
          spec:
            template:
              metadata:
                labels:
                  app: kube-scheduler
                annotations:
                  scheduler.alpha.kubernetes.io/critical-pod: ''
                  prometheus.io/scrape: "true"
                  prometheus.io/path: "/metrics"
                  prometheus.io/port: "10251"
              spec:
                nodeSelector:
                  control-plane: "true"
                hostNetwork: true
                containers:
                - name: kube-scheduler
                  image: {{.hyperkube_image_url}}
                  imagePullPolicy: IfNotPresent
                  command:
                  - /hyperkube
                  - scheduler
                  - --master=http://127.0.0.1:{{.kubernetes_apiserver_insecure_port}}
                  - --leader-elect=true
                  resources:
                    requests:
                      cpu: 100m
                  livenessProbe:
                    httpGet:
                      host: 127.0.0.1
                      path: /healthz
                      port: 10251
                    initialDelaySeconds: 15
                    timeoutSeconds: 15
                  readinessProbe:
                    failureThreshold: 3
                    httpGet:
                      host: 127.0.0.1
                      path: /healthz
                      port: 10251
                    initialDelaySeconds: 5
                    timeoutSeconds: 10
                    periodSeconds: 1

    - path: /etc/kubernetes/manifests/node-proxy.yaml
      filesystem: root
      contents:
        inline: |
          ---
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRole
          metadata:
            name: system:node-proxy
          rules:
          - apiGroups:
            - authentication.k8s.io
            resources:
            - tokenreviews
            verbs:
            - create
          - apiGroups:
            - authorization.k8s.io
            resources:
            - localsubjectaccessreviews
            - subjectaccessreviews
            verbs:
            - create
          - apiGroups:
            - ""
            resources:
            - services
            verbs:
            - get
            - list
            - watch
          - apiGroups:
            - ""
            resources:
            - nodes
            verbs:
            - create
            - get
            - list
            - watch
          - apiGroups:
            - ""
            resources:
            - nodes/status
            verbs:
            - patch
            - update
          - apiGroups:
            - ""
            resources:
            - nodes
            verbs:
            - delete
            - patch
            - update
          - apiGroups:
            - ""
            resources:
            - events
            verbs:
            - create
            - patch
            - update
          - apiGroups:
            - ""
            resources:
            - pods
            verbs:
            - get
            - list
            - watch
          - apiGroups:
            - ""
            resources:
            - pods
            verbs:
            - create
            - delete
          - apiGroups:
            - ""
            resources:
            - pods/status
            verbs:
            - update
          - apiGroups:
            - ""
            resources:
            - pods/eviction
            verbs:
            - create
          - apiGroups:
            - ""
            resources:
            - configmaps
            - secrets
            verbs:
            - get
          - apiGroups:
            - ""
            resources:
            - persistentvolumeclaims
            - persistentvolumes
            verbs:
            - get
          - apiGroups:
            - ""
            resources:
            - endpoints
            verbs:
            - get
          - apiGroups:
            - certificates.k8s.io
            resources:
            - certificatesigningrequests
            verbs:
            - create
            - get
            - list
            - watch

          - apiGroups:
            - ""
            resources:
            - endpoints
            - services
            verbs:
            - list
            - watch
          - apiGroups:
            - ""
            resources:
            - nodes
            verbs:
            - get
          - apiGroups:
            - ""
            resources:
            - events
            verbs:
            - create
            - patch
            - update
          ---
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRoleBinding
          metadata:
            name: container-linux
          roleRef:
            apiGroup: rbac.authorization.k8s.io
            kind: ClusterRole
            name: system:node-proxy
          subjects:
          - apiGroup: rbac.authorization.k8s.io
            kind: User
            name: node
          ---

    - path: /etc/kubernetes/manifests/kube-proxy.yaml
      filesystem: root
      contents:
        inline: |
          kind: Service
          apiVersion: v1
          metadata:
            name: kube-proxy
            namespace: kube-system
          spec:
            selector:
              app: kube-proxy
            ports:
            - port: 10256
              protocol: TCP
            clusterIP: "None"
          ---
          apiVersion: extensions/v1beta1
          kind: DaemonSet
          metadata:
            name: kube-proxy
            namespace: kube-system
          spec:
            template:
              metadata:
                labels:
                  app: kube-proxy
                annotations:
                  scheduler.alpha.kubernetes.io/critical-pod: ''
                  prometheus.io/scrape: "true"
                  prometheus.io/path: "/metrics"
                  prometheus.io/port: "10256"
              spec:
                hostNetwork: true
                containers:
                - name: kube-proxy
                  image: {{.hyperkube_image_url}}
                  imagePullPolicy: IfNotPresent
                  command:
                  - /hyperkube
                  - proxy
                  - --kubeconfig=/etc/kubernetes/kube-proxy/kubeconfig
                  - --masquerade-all
                  resources:
                    requests:
                      cpu: 100m
                  livenessProbe:
                    httpGet:
                      host: 127.0.0.1
                      path: /healthz
                      port: 10256
                    initialDelaySeconds: 15
                    timeoutSeconds: 15
                  readinessProbe:
                    failureThreshold: 3
                    httpGet:
                      host: 127.0.0.1
                      path: /healthz
                      port: 10256
                    initialDelaySeconds: 5
                    timeoutSeconds: 10
                    periodSeconds: 1
                  securityContext:
                    privileged: true
                  volumeMounts:
                  - name: secrets
                    mountPath: /etc/secrets
                    readOnly: true
                  - name: config
                    mountPath: /etc/kubernetes/kube-proxy
                    readOnly: true
                volumes:
                - name: secrets
                  hostPath:
                    path: /etc/vault.d/kubernetes
                - name: config
                  hostPath:
                    path: /etc/kubernetes/kube-proxy

    - path: /etc/kubernetes/kube-proxy/kubeconfig
      mode: 0400
      filesystem: root
      contents:
        inline: |
          apiVersion: v1
          kind: Config
          clusters:
            - cluster:
                server: http://127.0.0.1:8080
              name: kubernetes
          contexts:
            - context:
                cluster: kubernetes
                user: proxy
              name: proxy-to-kubernetes
          current-context: proxy-to-kubernetes
          users:
            - name: proxy

    - path: /var/lib/kubelet/kubeconfig
      mode: 0400
      filesystem: root
      contents:
        inline: |
          apiVersion: v1
          kind: Config
          clusters:
            - cluster:
                server: http://127.0.0.1:8080
              name: kubernetes
          contexts:
            - context:
                cluster: kubernetes
                user: kubelet
              name: kubelet-to-kubernetes
          current-context: kubelet-to-kubernetes
          users:
            - name: kubelet

    - path: /etc/metadata.env
      filesystem: root
      mode: 0644
      contents:
        inline: |
          # Configuration created by matchbox wrapped with Enjoliver
          # Mainly by the ~/app/configs.yaml, ~/app/configs.py, ~/app/sync.py
          # Retrieve the current configuration by calling the following URL:
          # curl '{{.api_uri}}/metadata?{{.request.raw_query}}' | sort
          API_URI='{{.api_uri}}'
          CNI='{{.cni}}'
          DNS_ATTR_DC='{{.dns_attr.dc}}'
          DNS_ATTR_DOMAIN='{{.dns_attr.domain}}'
          DNS_ATTR_POS='{{.dns_attr.pos}}'
          DNS_ATTR_RACK='{{.dns_attr.rack}}'
          DNS_ATTR_SHORTNAME='{{.dns_attr.shortname}}'
          ETCD_NAME='{{.etcd_name}}'
          FLEET_ETCD_ADVERTISE_CLIENT_URLS='{{.fleet_etcd_advertise_client_urls}}'
          FLEET_ETCD_CLIENT_PORT='{{.fleet_etcd_client_port}}'
          FLEET_ETCD_DATA_DIR='{{.fleet_etcd_data_dir}}'
          FLEET_ETCD_INITIAL_ADVERTISE_PEER_URLS='{{.fleet_etcd_initial_advertise_peer_urls}}'
          FLEET_ETCD_INITIAL_CLUSTER='{{.fleet_etcd_initial_cluster}}'
          FLEET_ETCD_MEMBER_CLIENT_URI_LIST='{{.fleet_etcd_member_client_uri_list}}'
          FLEET_ETCD_MEMBER_PEER_URI_LIST='{{.fleet_etcd_member_peer_uri_list}}'
          FLEET_ETCD_PEER_PORT='{{.fleet_etcd_peer_port}}'
          HOSTNAME='{{.hostname}}'
          HYPERKUBE_IMAGE_URL='{{.hyperkube_image_url}}'
          KUBERNETES_APISERVER_COUNT='{{.kubernetes_apiserver_count}}'
          KUBERNETES_APISERVER_INSECURE_PORT='{{.kubernetes_apiserver_insecure_port}}'
          KUBERNETES_ETCD_ADVERTISE_CLIENT_URLS='{{.kubernetes_etcd_advertise_client_urls}}'
          KUBERNETES_ETCD_CLIENT_PORT='{{.kubernetes_etcd_client_port}}'
          KUBERNETES_ETCD_DATA_DIR='{{.kubernetes_etcd_data_dir}}'
          KUBERNETES_ETCD_INITIAL_ADVERTISE_PEER_URLS='{{.kubernetes_etcd_initial_advertise_peer_urls}}'
          KUBERNETES_ETCD_INITIAL_CLUSTER='{{.kubernetes_etcd_initial_cluster}}'
          KUBERNETES_ETCD_MEMBER_CLIENT_URI_LIST='{{.kubernetes_etcd_member_client_uri_list}}'
          KUBERNETES_ETCD_MEMBER_PEER_URI_LIST='{{.kubernetes_etcd_member_peer_uri_list}}'
          KUBERNETES_ETCD_PEER_PORT='{{.kubernetes_etcd_peer_port}}'
          KUBERNETES_NODE_IP='{{.kubernetes_node_ip}}'
          KUBERNETES_NODE_NAME='{{.kubernetes_node_name}}'
          KUBERNETES_SERVICE_CLUSTER_IP_RANGE='{{.kubernetes_service_cluster_ip_range}}'
          KUBELET_HEALTHZ_PORT='{{.kubelet_healthz_port}}'
          MAC='{{.mac}}'
          NETWORK_CIDRV4='{{.network.cidrv4}}'
          NETWORK_GATEWAY='{{.network.gateway}}'
          NETWORK_IP='{{.network.ip_or_fqdn}}'
          REQUEST_QUERY_MAC='{{.request.query.mac}}'
          REQUEST_QUERY_UUID='{{.request.query.uuid}}'
          REQUEST_RAW_QUERY='{{.request.raw_query}}'
          ROLES='{{.roles}}'
          SELECTOR_MAC='{{.selector.mac}}'
          VAULT_ETCD_ADVERTISE_CLIENT_URLS='{{.fleet_etcd_advertise_client_urls}}'
          VAULT_ETCD_CLIENT_PORT='{{.fleet_etcd_client_port}}'
          VAULT_ETCD_DATA_DIR='{{.fleet_etcd_data_dir}}'
          VAULT_ETCD_INITIAL_ADVERTISE_PEER_URLS='{{.fleet_etcd_initial_advertise_peer_urls}}'
          VAULT_ETCD_INITIAL_CLUSTER='{{.fleet_etcd_initial_cluster}}'
          VAULT_ETCD_MEMBER_CLIENT_URI_LIST='{{.fleet_etcd_member_client_uri_list}}'
          VAULT_ETCD_MEMBER_PEER_URI_LIST='{{.fleet_etcd_member_peer_uri_list}}'
          VAULT_ETCD_PEER_PORT='{{.fleet_etcd_peer_port}}'
          VAULT_ETCD_ADVERTISE_CLIENT_URLS='{{.vault_etcd_advertise_client_urls}}'
          VAULT_ETCD_CLIENT_PORT='{{.vault_etcd_client_port}}'
          VAULT_ETCD_DATA_DIR='{{.vault_etcd_data_dir}}'
          VAULT_ETCD_INITIAL_ADVERTISE_PEER_URLS='{{.vault_etcd_initial_advertise_peer_urls}}'
          VAULT_ETCD_INITIAL_CLUSTER='{{.vault_etcd_initial_cluster}}'
          VAULT_ETCD_MEMBER_CLIENT_URI_LIST='{{.vault_etcd_member_client_uri_list}}'
          VAULT_ETCD_MEMBER_PEER_URI_LIST='{{.vault_etcd_member_peer_uri_list}}'
          VAULT_ETCD_PEER_PORT='{{.vault_etcd_peer_port}}'
          VAULT_PORT='{{.vault_port}}'

    - path: /etc/hostname
      filesystem: root
      mode: 0644
      contents:
        inline: |
          {{.hostname}}

    - path: /etc/rkt/paths.d/paths.json
      filesystem: root
      mode: 0644
      contents:
        inline: |
          {
            "rktKind": "paths",
            "rktVersion": "v1",
            "stage1-images": "/usr/lib/rkt/stage1-images"
          }

    - path: /etc/rkt/stage1.d/coreos.json
      filesystem: root
      mode: 0644
      contents:
        inline: |
          {
              "rktKind": "stage1",
              "rktVersion": "v1",
              "name": "coreos.com/rkt/stage1-coreos",
              "version": "v1.27.0",
              "location": "/usr/lib/rkt/stage1-images/stage1-coreos.aci"
          }

    - path: /etc/rkt/net.d/10-k8s.conf
      filesystem: root
      mode: 0644
      contents:
        inline: |
          {
            "name": "bond0",
            "type": "macvlan",
            "master": "bond0",
            "ipam": {{ .cni }}
          }

    - path: /etc/vault.d/config.hcl.insecure
      filesystem: root
      mode: 0644
      contents:
        inline: |
          storage "etcd" {
            address  = "http://127.0.0.1:{{.vault_etcd_client_port}}"
            etcd_api = "v3"
            sync = "false"
          }
          listener "tcp" {
            address     = "127.0.0.1:{{.vault_port}}"
            tls_disable = 1
          }

    - path: /etc/vault.d/config.hcl.secure
      filesystem: root
      mode: 0644
      contents:
        inline: |
          storage "etcd" {
            address  = "http://127.0.0.1:{{.vault_etcd_client_port}}"
            etcd_api = "v3"
            sync = "false"
          }
          listener "tcp" {
            address     = "{{.network.ip_or_fqdn}}:{{.vault_port}}"
            tls_cert_file = "/etc/vault.d/vault/server.certificate"
            tls_key_file  = "/etc/vault.d/vault/server.private_key"
          }
          listener "tcp" {
            address     = "127.0.0.1:{{.vault_port}}"
            tls_disable = 1
          }

    - path: /etc/fleet/fleet.conf
      mode: 0644
      filesystem: root
      contents:
        inline: |
          etcd_servers = [ http://127.0.0.1:{{.fleet_etcd_client_port}},{{ .fleet_etcd_member_client_uri_list }} ]
          metadata = "name={{.dns_attr.shortname}},diskProfile={{.disk_profile}}"
          etcd_cafile=/etc/vault.d/etcd-fleet/client.issuing_ca
          etcd_certfile=/etc/vault.d/etcd-fleet/client.certificate
          etcd_keyfile=/etc/vault.d/etcd-fleet/client.private_key
          agent_ttl = 120s
          engine_reconcile_interval = 1
          etcd_request_timeout = 10

    - path: /etc/modprobe.d/bonding.conf
      mode: 0644
      filesystem: root
      contents:
        inline: |
          options bonding mode=1 miimon=100

    - path: /etc/hosts
      mode: 0644
      filesystem: root
      contents:
        inline: |
          127.0.0.1	localhost
          {{ if index . "etc_hosts" }}
          {{ range $element := .etc_hosts }}
          {{$element}}
          {{end}}
          {{end}}

    - path: /etc/coreos/update.conf
      mode: 0644
      filesystem: root
      contents:
        inline: |
          GROUP=stable
          REBOOT_STRATEGY=off

    - path: /etc/systemd/timesyncd.conf
      mode: 0644
      filesystem: root
      contents:
        inline: |
          [Time]
          NTP={{.ntp}}
          FallbackNTP={{.fallbackntp}}

    - path: /etc/systemd/resolved.conf
      mode: 0644
      filesystem: root
      contents:
        inline: |
          [Resolve]
          DNS={{.nameservers}}
          LLMNR=false

    - path: /var/log/journal/.keep
      mode: 0644
      filesystem: root

    - path: /etc/etcd-vault
      mode: 0644
      filesystem: root
      contents:
        inline: |
          ETCD_NAME={{.etcd_name}}
          ETCD_INITIAL_CLUSTER={{.vault_etcd_initial_cluster}}
          ETCD_ADVERTISE_CLIENT_URLS="{{.vault_etcd_advertise_client_urls}}"
          ETCD_INITIAL_ADVERTISE_PEER_URLS={{.vault_etcd_initial_advertise_peer_urls}}
          ETCD_MEMBER_CLIENT_URI_LIST={{.vault_etcd_member_client_uri_list}}
          ETCD_MEMBER_PEER_URI_LIST={{.vault_etcd_member_peer_uri_list}}
          ETCD_DATA_DIR={{.vault_etcd_data_dir}}
          ETCD_LISTEN_CLIENT_URLS="http://127.0.0.1:{{.vault_etcd_client_port}},https://{{.network.ip_or_fqdn}}:{{.vault_etcd_client_port}}"
          ETCD_LISTEN_PEER_URLS="https://0.0.0.0:{{.vault_etcd_peer_port}}"

          NETWORK_IP={{.network.ip_or_fqdn}}
          ETCD_FLAGS="--insecure-transport=false --insecure-skip-tls-verify=true"

    - path: /etc/etcd-kubernetes
      mode: 0644
      filesystem: root
      contents:
        inline: |
          ETCD_NAME={{.etcd_name}}
          ETCD_INITIAL_CLUSTER={{.kubernetes_etcd_initial_cluster}}
          ETCD_ADVERTISE_CLIENT_URLS={{.kubernetes_etcd_advertise_client_urls}}
          ETCD_INITIAL_ADVERTISE_PEER_URLS={{.kubernetes_etcd_initial_advertise_peer_urls}}
          ETCD_MEMBER_CLIENT_URI_LIST={{.kubernetes_etcd_member_client_uri_list}}
          ETCD_MEMBER_PEER_URI_LIST={{.kubernetes_etcd_member_peer_uri_list}}
          ETCD_DATA_DIR={{.kubernetes_etcd_data_dir}}
          ETCD_LISTEN_CLIENT_URLS="http://127.0.0.1:{{.kubernetes_etcd_client_port}},https://{{.network.ip_or_fqdn}}:{{.kubernetes_etcd_client_port}}"
          ETCD_LISTEN_PEER_URLS="https://0.0.0.0:{{.kubernetes_etcd_peer_port}}"

          NETWORK_IP={{.network.ip_or_fqdn}}
          ETCD_FLAGS="--cert=/etc/vault.d/etcd-kubernetes/client.certificate \
          --key=/etc/vault.d/etcd-kubernetes/client.private_key \
          --cacert=/etc/vault.d/etcd-kubernetes/client.issuing_ca \
          --insecure-skip-tls-verify=false \
          --insecure-transport=false"

    - path: /etc/etcd-fleet
      mode: 0644
      filesystem: root
      contents:
        inline: |
          ETCD_NAME={{.etcd_name}}
          ETCD_INITIAL_CLUSTER={{.fleet_etcd_initial_cluster}}
          ETCD_ADVERTISE_CLIENT_URLS={{.fleet_etcd_advertise_client_urls}}
          ETCD_INITIAL_ADVERTISE_PEER_URLS={{.fleet_etcd_initial_advertise_peer_urls}}
          ETCD_MEMBER_CLIENT_URI_LIST={{.fleet_etcd_member_client_uri_list}}
          ETCD_MEMBER_PEER_URI_LIST={{.fleet_etcd_member_peer_uri_list}}
          ETCD_DATA_DIR={{.fleet_etcd_data_dir}}
          ETCD_LISTEN_CLIENT_URLS="http://127.0.0.1:{{.fleet_etcd_client_port}},https://{{.network.ip_or_fqdn}}:{{.fleet_etcd_client_port}},http://{{.network.ip_or_fqdn}}:2379"
          ETCD_LISTEN_PEER_URLS="https://0.0.0.0:{{.fleet_etcd_peer_port}}"

          NETWORK_IP={{.network.ip_or_fqdn}}
          ETCD_FLAGS="--cert=/etc/vault.d/etcd-fleet/client.certificate \
          --key=/etc/vault.d/etcd-fleet/client.private_key \
          --cacert=/etc/vault.d/etcd-fleet/client.issuing_ca \
          --insecure-skip-tls-verify=false \
          --insecure-transport=false"

    - path: /opt/bin/etcdctl3
      filesystem: root
      mode: 0544
      contents:
        inline: |
          #!/bin/bash
          ETCDCTL_API=3 exec /usr/bin/etcdctl $@

    - path: /opt/bin/ping-one-cp
      filesystem: root
      mode: 0544
      contents:
        inline: |
          #!/bin/bash

          # try to ping other etcd_member_kubernetes_control_plane_ip_list
          # exit 0 when at least one reply or if we are in single node
          # exit 2 if no one are reachable
          set -x

          echo -n {{ .etcd_member_kubernetes_control_plane_ip_list }} | grep -c , || exit 0

          for i in {0..2}
          do
            for ip in $(echo -n {{ .etcd_member_kubernetes_control_plane_ip_list }} | tr ',' '\n' | grep -v {{.network.ip_or_fqdn}} | shuf )
            do
              ping -c 1 ${ip} && exit 0
              sleep ${i}
            done
          done

          exit 2

    - path: /opt/bin/etcd-operator
      filesystem: root
      mode: 0544
      contents:
        inline: |
          #!/usr/bin/env bash
          set -o pipefail ; set -ex
          export PATH=/opt/bin/:${PATH}
          export ETCDCTL_API=3

          env | grep "^ETCD"
          echo ${ETCD_ENV_FILE}

          set +e
          # Check if the data-dir is empty: First boot
          if [[ ! -d ${ETCD_DATA_DIR}/member ]]
          then

              echo "${ETCD_DATA_DIR}/member is empty"
              # Check if there is an existing cluster
              etcdctl ${ETCD_FLAGS} --endpoints ${ETCD_MEMBER_CLIENT_URI_LIST} endpoint status
              RET=$?

              if [[ ${RET} -eq 128 ]]
              then
                echo "certs in ETCD_FLAGS=${ETCD_FLAGS} unavailables" >&2
                exit 2

              elif [[ ${RET} -eq 1 ]]
              then
                echo "Existing cluster"
                ETCD_INITIAL_CLUSTER_STATE=existing

                # Check if this new member make a replacement
                REMOVE=$(etcdctl ${ETCD_FLAGS} --endpoints ${ETCD_MEMBER_CLIENT_URI_LIST} member list | \
                  grep {{.network.ip_or_fqdn}}: | cut -f1 -d ',')

                if [[ x"$REMOVE" != "x" ]]
                then
                  echo "Remove ${REMOVE}"
                  etcdctl ${ETCD_FLAGS} --endpoints ${ETCD_MEMBER_CLIENT_URI_LIST} member remove ${REMOVE}
                fi

                echo "Extend original cluster"
                ETCD_INITIAL_CLUSTER=$(etcdctl ${ETCD_FLAGS} --endpoints ${ETCD_MEMBER_CLIENT_URI_LIST} member add \
                  ${ETCD_NAME} --peer-urls=${ETCD_INITIAL_ADVERTISE_PEER_URLS} | grep "ETCD_INITIAL_CLUSTER=" | \
                  sed 's/ETCD_INITIAL_CLUSTER=//')

                if [[ -z $ETCD_INITIAL_CLUSTER ]]
                then
                  echo "Fail to extend original cluster: ETCD_INITIAL_CLUSTER=${ETCD_INITIAL_CLUSTER}"
                fi
              else
                ETCD_INITIAL_CLUSTER_STATE=new
                mkdir -pv ${ETCD_DATA_DIR}
              fi

            else
              echo "Already some data in ${ETCD_DATA_DIR}"
              cat ${ETCD_ENV_FILE}
              exit $?
          fi

          set -e

          if [[ -z ${ETCD_INITIAL_CLUSTER} ]]
          then
              echo '${ETCD_INITIAL_CLUSTER} is empty'
              exit 2
          fi

          cat << EOF | tee ${ETCD_ENV_FILE}
          ETCD_INITIAL_CLUSTER_STATE=$ETCD_INITIAL_CLUSTER_STATE
          ETCD_INITIAL_CLUSTER=$ETCD_INITIAL_CLUSTER
          ETCD_ADVERTISE_CLIENT_URLS=$ETCD_ADVERTISE_CLIENT_URLS
          ETCD_INITIAL_ADVERTISE_PEER_URLS=$ETCD_INITIAL_ADVERTISE_PEER_URLS
          ETCD_LISTEN_CLIENT_URLS=$ETCD_LISTEN_CLIENT_URLS
          ETCD_LISTEN_PEER_URLS=$ETCD_LISTEN_PEER_URLS
          ETCD_NAME=$ETCD_NAME
          ETCD_DATA_DIR=$ETCD_DATA_DIR
          EOF

    - path: /etc/profile.d/common.sh
      mode: 0755
      filesystem: root
      contents:
        inline: |
           export PATH=/opt/bin:$PATH
           export PRIVATE_IPV4={{.network.ip_or_fqdn}}
           export VAULT_ADDR=http://127.0.0.1:{{.vault_port}}

    - path: /etc/modules-load.d/network.conf
      mode: 0644
      filesystem: root
      contents:
        inline: |
          ip_tables
          iptable_nat
          nf_nat

    - path: /opt/bin/lifecycle-update
      mode: 0544
      filesystem: root
      contents:
        inline: |
          #!/usr/bin/env bash

          set -e
          set -o pipefail

          curl -f {{.api_uri}}/healthz
          STATUS=$(curl -f -XPOST "{{.api_uri}}/lifecycle/ignition/{{.request.raw_query}}" \
                -d @/usr/share/oem/coreos-install.json \
                -H "Content-Type: application/json" \
                -w "%{http_code}" -o /dev/null)

          set +e
          if [[ ${STATUS} -ne 210 ]]
          then
              PRESET=/run/systemd-preset-all
              echo "Checking if preset on hold"
              grep -c "^Created symlink" ${PRESET}
              if [[ $? -ne 0 ]]
              then
                echo "Nothing to do: ${STATUS}, no preset in ${PRESET}"
                exit 0
              fi
              cat ${PRESET} || {
                echo "Nothing to do: ${STATUS}, no file preset ${PRESET}"
                exit 0
              }
          fi

          set -xe

          curl -f "{{.api_uri}}/ignition?{{.request.raw_query}}" -o /tmp/coreos-install.json
          cat /tmp/coreos-install.json | jq -e . > /dev/null

          STRATEGY=$(curl -f "{{.api_uri}}/lifecycle/rolling/{{.request.raw_query}}" | jq -re .strategy ) || {
            exit 0
          }

          echo "Update enable by strategy: ${STRATEGY}"

          echo "locksmithctl operations..."
          locksmithctl -endpoint http://127.0.0.1:{{.fleet_etcd_client_port}} status
          locksmithctl -endpoint http://127.0.0.1:{{.fleet_etcd_client_port}} unlock "{{.request.raw_query}}" || true
          locksmithctl -endpoint http://127.0.0.1:{{.fleet_etcd_client_port}} lock "{{.request.raw_query}}"

          echo "stopping fleet..."
          systemctl stop fleet.service

          echo "draining node..."
          kubectl -s http://127.0.0.1:{{.kubernetes_apiserver_insecure_port}} \
            drain {{.kubernetes_node_name}} --force --ignore-daemonsets --delete-local-data

          set +e
          for service in kubelet kube-apiserver etcd3@kubernetes etcd3@fleet vault etcd3@vault
          do
            systemctl stop ${service}.service
          done

          set -e

          DEVICE=/dev/sda
          DISK_GUID="00000000-0000-0000-0000-000000000001"
          sgdisk --disk-guid=${DISK_GUID} ${DEVICE}
          cgpt show -v ${DEVICE} | grep -c ${DISK_GUID}

          if [[ ${STRATEGY} == "kexec" ]]
          then
            kexec --reuse-cmdline \
                  --append="coreos.first_boot=1 coreos.randomize_disk_guid=${DISK_GUID}" \
                  -l /usr/boot/vmlinuz
          fi

          cp -v /tmp/coreos-install.json /usr/share/oem/coreos-install.json

          systemctl ${STRATEGY}


    - path: /opt/bin/lifecycle-ready
      mode: 0544
      filesystem: root
      contents:
        inline: |
          #!/usr/bin/env bash
          export PATH=/usr/bin:/opt/bin/:${PATH}
          export VAULT_ADDR=http://127.0.0.1:{{.vault_port}}

          set -o pipefail

          function retry {
            until $@
            do
              echo "waiting for $@"
              sleep 1
            done
          }

          export ETCDCTL_API=3
          retry etcdctl \
             --insecure-skip-tls-verify=true \
             --insecure-transport=false \
             --endpoints http://127.0.0.1:{{.vault_etcd_client_port}} endpoint status

          retry vault status

          retry etcdctl \
             --cert=/etc/vault.d/etcd-kubernetes/client.certificate \
             --key=/etc/vault.d/etcd-kubernetes/client.private_key \
             --cacert=/etc/vault.d/etcd-kubernetes/client.issuing_ca \
             --endpoints http://127.0.0.1:{{.kubernetes_etcd_client_port}} endpoint status

          retry etcdctl \
             --cert=/etc/vault.d/etcd-fleet/client.certificate \
             --key=/etc/vault.d/etcd-fleet/client.private_key \
             --cacert=/etc/vault.d/etcd-fleet/client.issuing_ca \
             --endpoints http://127.0.0.1:{{.fleet_etcd_client_port}} endpoint status

          unset ETCDCTL_API
          retry etcdctl \
             --cert-file /etc/vault.d/etcd-fleet/client.certificate \
             --key-file /etc/vault.d/etcd-fleet/client.private_key \
             --ca-file /etc/vault.d/etcd-fleet/client.issuing_ca \
             --endpoints http://127.0.0.1:{{.fleet_etcd_client_port}} cluster-health

          retry kubectl -s http://127.0.0.1:{{.kubernetes_apiserver_insecure_port}} cluster-info

          retry curl -fs http://127.0.0.1:10248/healthz

          retry fleetctl --endpoint http://127.0.0.1:{{.fleet_etcd_client_port}} \
            --driver etcd list-machines \
            --fields ip --no-legend | grep -c {{.network.ip}}

          retry fleetctl \
            --ca-file /etc/vault.d/etcd-fleet/client.issuing_ca \
            --cert-file /etc/vault.d/etcd-fleet/client.certificate \
            --key-file /etc/vault.d/etcd-fleet/client.private_key \
            --driver etcd --endpoint https://{{.network.ip_or_fqdn}}:{{.fleet_etcd_client_port}},{{.fleet_etcd_member_client_uri_list}} \
            list-machines --fields ip --no-legend | grep -c {{.network.ip}}

          retry kubectl -s http://127.0.0.1:{{.kubernetes_apiserver_insecure_port}} uncordon {{.kubernetes_node_name}}

          systemctl preset-all 2>&1 | tee -a /run/systemd-preset-all

          set -e

          echo "remove the lock"
          locksmithctl -endpoint http://127.0.0.1:{{.fleet_etcd_client_port}} unlock "{{.request.raw_query}}" || true
          echo "$(hostname) {{.network.ip_or_fqdn}} is ready"

    - path: /opt/bin/vault-config
      mode: 0544
      filesystem: root
      contents:
        inline: |
          #!/bin/bash

          KEY="https://{{.network.ip_or_fqdn}}:{{.vault_port}}"
          ls -l /etc/vault.d/vault/server.certificate /etc/vault.d/vault/server.private_key || {
            curl -f "http://127.0.0.1:{{.vault_etcd_client_port}}/v2/keys/initier?prevExist=false" -XPUT -d "value=${KEY}" || \
              curl -f "http://127.0.0.1:{{.vault_etcd_client_port}}/v2/keys/initier?prevValue=${KEY}" -XPUT -d "value=${KEY}" && {
                cp -v /etc/vault.d/config.hcl.insecure /etc/vault.d/config.hcl
                echo ${KEY} >> /etc/vault.d/initier
            }
            exit 0
          }
          cp -v /etc/vault.d/config.hcl.secure /etc/vault.d/config.hcl

    - path: /opt/bin/vault-init
      mode: 0544
      filesystem: root
      contents:
        inline: |
          #!/usr/bin/env bash

          export VAULT_ADDR="http://127.0.0.1:{{.vault_port}}"
          export PATH=/opt/bin:${PATH}
          function retry {
            until $@
            do
              echo "waiting for $@"
              sleep 1
            done
          }
          export ETCDCTL_API=3
          retry etcdctl --endpoints http://127.0.0.1:{{.vault_etcd_client_port}} endpoint health
          unset ETCDCTL_API

          echo "Vault have to respond and may be sealed:"
          curl http://127.0.0.1:{{.vault_port}}/v1/ | jq -re .errors || exit 1

          vault init -check && {
            echo "GET vault: https://{{.network.ip_or_fqdn}}:{{.vault_port}}/v1/"
            curl --insecure https://{{.network.ip_or_fqdn}}:{{.vault_port}}/v1/ || {
              echo "vault already init but not running with TLS"
              retry ls -l /etc/vault.d/vault/server.certificate /etc/vault.d/vault/server.private_key
              systemctl restart vault
              echo "restarted vault"
            }
            exit 0
          }

          ls -l /etc/vault.d/initier || {
            echo "not eligible as initier"
            exit 1
          }

          KEY=/etc/vault.d/keys
          echo "${KEY} may have to NOT be here"
          ls -l ${KEY} && {
            echo "${KEY} already here, have to be empty"
            wc -l ${KEY}
            test -s ${KEY} && exit 1
          }

          set -e
          set -o pipefail

          echo "vault init with ${KEY}"
          umask 077
          vault init -key-shares=1 -key-threshold=1 > ${KEY}

          UNSEALKEY1=$(grep "Unseal Key 1:" ${KEY} | cut -f4 -d ' ')
          ROOTTOKEN=$(grep "Initial Root Token:" ${KEY} | cut -f4 -d ' ')

          retry vault unseal ${UNSEALKEY1}
          retry vault auth ${ROOTTOKEN} >> /etc/vault.d/vault.log

          echo "create certs for vault himself"
          vault mount -path pki/vault pki
          vault mount-tune -max-lease-ttl=87600h pki/vault
          vault write pki/vault/root/generate/internal common_name=vault ttl=87600h # 10y
          vault write pki/vault/roles/server allow_any_name=true max_ttl=43800h # 2y

          vault policy-write vault/server - << EOF
          path "pki/vault/issue/server" {
            policy = "write"
          }
          EOF

          vault token-create -format=json \
            -display-name vault/server \
            -ttl="8760h" -orphan -period="8760h" \
            -policy="vault/server" | jq -er .auth.client_token | \
              etcdctl --no-sync --endpoints http://127.0.0.1:{{.vault_etcd_client_port}} \
                set token/vault/server >> /etc/vault.d/vault.log

          vault write -format=json pki/vault/issue/server \
            ttl=17520h \
            common_name={{.network.ip_or_fqdn}} \
            ip_sans={{.network.ip_or_fqdn}} \
              > /etc/vault.d/vault/server-pki.json

          for item in certificate issuing_ca private_key
          do
            jq -re .data.${item} /etc/vault.d/vault/server-pki.json > /etc/vault.d/vault/server.${item}
          done

          cat /etc/vault.d/vault/server.issuing_ca | etcdctl --no-sync --endpoints http://127.0.0.1:{{.vault_etcd_client_port}} \
            set pki/vault/server.issuing_ca

          systemctl restart vault

          retry vault unseal ${UNSEALKEY1}
          retry vault auth ${ROOTTOKEN} >> /etc/vault.d/vault.log

          echo "init cluster task"
          for ETCD_ROLE in etcd-kubernetes etcd-fleet
          do
            echo "task for ${ETCD_ROLE}"
            vault mount -path pki/${ETCD_ROLE} pki
            vault mount-tune -max-lease-ttl=87600h pki/${ETCD_ROLE}
            vault write pki/${ETCD_ROLE}/root/generate/internal common_name=${ETCD_ROLE} ttl=87600h # 10y
            vault write pki/${ETCD_ROLE}/roles/peer allow_any_name=true max_ttl=43800h # 2y
            vault write pki/${ETCD_ROLE}/roles/client allow_any_name=true max_ttl=43800h # 2y

            vault policy-write ${ETCD_ROLE}/peer - << EOF
            path "pki/${ETCD_ROLE}/issue/peer" {
              policy = "write"
            }
          EOF

            vault policy-write ${ETCD_ROLE}/client - << EOF
            path "pki/${ETCD_ROLE}/issue/client" {
              policy = "write"
            }
          EOF

            echo "token create ${ETCD_ROLE}/peer" | tee -a /etc/vault.d/vault.log
            vault token-create -format=json \
              -display-name ${ETCD_ROLE}/peer \
              -ttl="8760h" -orphan -period="8760h" \
              -policy="${ETCD_ROLE}/peer" | jq -er .auth.client_token | \
                etcdctl --no-sync --endpoints http://127.0.0.1:{{.vault_etcd_client_port}} \
                  set token/${ETCD_ROLE}/peer >> /etc/vault.d/vault.log

            echo "token create ${ETCD_ROLE}/client" | tee -a /etc/vault.d/vault.log
            vault token-create -format=json \
              -display-name ${ETCD_ROLE}/client \
              -ttl="8760h" -orphan -period="8760h" \
              -policy="${ETCD_ROLE}/client" | jq -er .auth.client_token | \
                etcdctl --no-sync --endpoints http://127.0.0.1:{{.vault_etcd_client_port}} \
                  set token/${ETCD_ROLE}/client >> /etc/vault.d/vault.log
          done

          echo "task for kubernetes"
          openssl genrsa 2048 > /etc/vault.d/service-accounts.rsa
          vault write secret/kubernetes/service-accounts key=@/etc/vault.d/service-accounts.rsa

          vault mount -path pki/kubernetes pki
          vault mount-tune -max-lease-ttl=87600h pki/kubernetes
          vault write pki/kubernetes/root/generate/internal common_name=kubernetes ttl=87600h # 10y
          vault write pki/kubernetes/roles/kube-apiserver allow_any_name=true max_ttl=43800h # 2y
          vault write pki/kubernetes/roles/kubelet allow_any_name=true max_ttl=43800h # 2y
          vault write pki/kubernetes/roles/kubectl allow_any_name=true organization="system:masters" max_ttl=1440h # 60j

          vault policy-write kubernetes/kube-apiserver - << EOF
          path "pki/kubernetes/issue/kube-apiserver" {
            policy = "write"
          }
          path "secret/kubernetes/service-accounts" {
            policy = "read"
          }
          EOF

          vault policy-write kubernetes/kubelet - << EOF
          path "pki/kubernetes/issue/kubelet" {
            policy = "write"
          }
          EOF

          vault policy-write kubernetes/kubectl - << EOF
          path "pki/kubernetes/issue/kubectl" {
            policy = "write"
          }
          EOF

          echo "token create kube-apiserver" | tee -a /etc/vault.d/vault.log
          vault token-create -format=json \
            -display-name kubernetes/kube-apiserver \
            -ttl="8760h" -orphan -period="8760h" \
            -policy="kubernetes/kube-apiserver" | jq -er .auth.client_token | \
              etcdctl --no-sync --endpoints http://127.0.0.1:{{.vault_etcd_client_port}} \
                set token/kubernetes/kube-apiserver >> /etc/vault.d/vault.log

          echo "token create kubelet" | tee -a /etc/vault.d/vault.log
          vault token-create -format=json \
            -display-name kubernetes/kubelet \
            -ttl="8760h" -orphan -period="8760h" \
            -policy="kubernetes/kubelet" | jq -er .auth.client_token | \
              etcdctl --no-sync --endpoints http://127.0.0.1:{{.vault_etcd_client_port}} \
                set token/kubernetes/kubelet >> /etc/vault.d/vault.log

          echo "token create kubectl" | tee -a /etc/vault.d/vault.log
          vault token-create -format=json \
            -display-name kubernetes/kubectl \
            -ttl="8760h" -orphan -period="8760h" \
            -policy="kubernetes/kubectl" | jq -er .auth.client_token | \
              etcdctl --no-sync --endpoints http://127.0.0.1:{{.vault_etcd_client_port}} \
                set token/kubernetes/kubectl >> /etc/vault.d/vault.log

          echo "finished $0" | tee -a /etc/vault.d/vault.log

    - path: /opt/bin/vault-token
      mode: 0544
      filesystem: root
      contents:
        inline: |
          #!/usr/bin/env bash

          export PATH=/opt/bin:${PATH}
          set -o pipefail

          test ${1} || {
            echo 'missing $1 eg: etcd-kubernetes'
            exit 2
          }
          test ${2} || {
            echo 'missing $2 eg: peer'
            exit 2
          }

          while true
          do
            for ve in $(echo -n {{ .vault_etcd_member_client_uri_list }} | tr ',' '\n' | shuf)
            do
              REQ="${ve}/v2/keys/token/${1}/${2}"
              echo ${REQ}

              TOKEN=$(curl --insecure -fL ${REQ} | jq -re .node.value)
              if [[ $? -eq 0 ]]
              then
                echo "TOKEN=${TOKEN}" > /etc/vault.d/${1}/${2}.new

                cmp /etc/vault.d/${1}/${2}.token /etc/vault.d/${1}/${2}.new
                if [[ $? -eq 0 ]]
                then
                  echo "/etc/vault.d/${1}/${2}.token is up-to-date"
                  exit 0
                else
                  mv -v /etc/vault.d/${1}/${2}.new /etc/vault.d/${1}/${2}.token
                  exit 0
                fi
              fi
            done
            sleep 5
          done

    - path: /opt/bin/vault-fetch-ca
      mode: 0544
      filesystem: root
      contents:
        inline: |
          #!/usr/bin/env bash

          export PATH=/opt/bin:${PATH}
          export VAULT_CAPATH=/etc/vault.d/vault/server.issuing_ca
          set -o pipefail

          # TODO consul-template or check if renew
          test -s ${VAULT_CAPATH} || {
          for ve in $(echo -n {{ .vault_ip_list }} | tr ',' '\n' | shuf)
          do
              REQ=https://${ve}:{{.vault_etcd_client_port}}/v2/keys/pki/vault/server.issuing_ca
              echo "GET ${REQ}"
              curl -fs --insecure ${REQ} \
                | jq -re .node.value > ${VAULT_CAPATH} && {
                  ln -svf ${VAULT_CAPATH} /etc/ssl/certs/vault
                  ls -l ${VAULT_CAPATH} /etc/ssl/certs/vault
                  exit $?
              }
              sleep 5
          done
          }
          exit 2

    - path: /opt/bin/vault-pki-issue
      mode: 0544
      filesystem: root
      contents:
        inline: |
          #!/usr/bin/env bash

          set -x

          export PATH=/opt/bin:${PATH}
          export VAULT_CAPATH=/etc/vault.d/vault/server.issuing_ca
          set -o pipefail

          test ${1} || {
            echo 'missing $1 eg: etcd-kubernetes'
            exit 2
          }
          test ${2} || {
            echo 'missing $2 eg: peer'
            exit 2
          }
          test ${TOKEN} || {
            echo 'missing ${TOKEN}'
            exit 2
          }

          # TODO consul-template or check if renew
          test -s /etc/vault.d/${1}/${2}-pki.json && exit 0
          test -s ${VAULT_CAPATH} || {
            /opt/bin/vault-fetch-ca || exit $?
          }

          # The common_name default is the ip address
          test ${COMMON_NAME} || {
            COMMON_NAME={{.network.ip_or_fqdn}}
            echo "setting COMMON_NAME=${COMMON_NAME}"
          }

          while true
          do
            for ve in $(echo -n {{ .vault_ip_list }} | tr ',' '\n' | shuf)
            do
              REQ="https://${ve}:{{.vault_port}}/v1/pki/${1}/issue/${2}"
              echo "POST ${REQ}"
              curl ${REQ} \
                --cacert ${VAULT_CAPATH} -Lfs \
                --header "X-Vault-Token: ${TOKEN}" \
                -XPOST --data "{
                  \"common_name\": \"${COMMON_NAME}\",
                  \"ttl\": \"17520h\",
                  \"ip_sans\": \"{{.network.ip_or_fqdn}},127.0.0.1,{{.kubernetes_service_cluster_ip}}\"
                }" > /etc/vault.d/${1}/${2}-pki.json
              if [[ $? -eq 0 ]]
              then
                for item in certificate issuing_ca private_key
                do
                  jq -re .data.${item} /etc/vault.d/${1}/${2}-pki.json > /etc/vault.d/${1}/${2}.${item} || exit 2
                done
                exit 0
              fi
            done
            sleep 5
          done

    - path: /opt/bin/vault-secret
      mode: 0544
      filesystem: root
      contents:
        inline: |
          #!/usr/bin/env bash

          export PATH=/opt/bin:${PATH}
          export VAULT_CAPATH=/etc/vault.d/vault/server.issuing_ca
          set -o pipefail

          test ${1} || {
            echo 'missing $1 eg: kubernetes'
            exit 2
          }
          test ${2} || {
            echo 'missing $2 eg: service-accounts'
            exit 2
          }
          test ${TOKEN} || {
            echo 'missing ${TOKEN}'
            exit 2
          }

          # TODO consul-template or check if renew
          test -s /etc/vault.d/${1}/${2}-secret.json && exit 0
          test -s ${VAULT_CAPATH} || {
            /opt/bin/vault-fetch-ca || exit $?
          }

          while true
          do
            for ve in $(echo -n {{ .vault_ip_list }} | tr ',' '\n' | shuf)
            do
              REQ="https://${ve}:{{.vault_port}}/v1/secret/${1}/${2}"
              echo "GET ${REQ}"
              curl ${REQ} -Lfs \
                --cacert ${VAULT_CAPATH} \
                --header "X-Vault-Token: ${TOKEN}" > /etc/vault.d/${1}/${2}-secret.json
              if [[ $? -eq 0 ]]
              then
                for item in key
                do
                  jq -re .data.${item} /etc/vault.d/${1}/${2}-secret.json > /etc/vault.d/${1}/${2}.${item} || exit 2
                done
                exit 0
              fi
            done
            sleep 5
          done

    - path: /etc/vault.d/vault/.keep
      mode: 0600
      filesystem: root

    - path: /opt/bin/create-control-plane
      mode: 0544
      filesystem: root
      contents:
        inline: |
          #!/bin/bash -x

          kubectl -s http://127.0.0.1:{{.kubernetes_apiserver_insecure_port}} get -f /etc/kubernetes/manifests && {
            kubectl -s http://127.0.0.1:{{.kubernetes_apiserver_insecure_port}} apply -f /etc/kubernetes/manifests
            exit $?
          }
          kubectl -s http://127.0.0.1:{{.kubernetes_apiserver_insecure_port}} create --save-config -f /etc/kubernetes/manifests

    - path: /opt/bin/rbd
      mode: 0555
      filesystem: root
      contents:
        inline: |
          #!/bin/bash
          # LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/rbd/lib /opt/rbd/bin/rbd
          mkdir -p /etc/ceph
          rkt --insecure-options=all run \
            --volume data,kind=host,source=/etc/ceph \
            --mount volume=data,target=/etc/ceph \
            --interactive {{.cephtools_image_url}} -- $@

    - path: /etc/profile.d/alias.sh
      mode: 0755
      filesystem: root
      contents:
        inline: |
          if [[ $- != *i* ]] ; then
                   return
          fi
          alias ls='ls --color=auto'
          alias la='ls -la'
          alias ll='ls -ll'

networkd:
  units:
    - name: lo.network
      contents: |
        [Match]
        Name=lo
        [Network]
        Address={{.kubernetes_service_cluster_ip_range}}

    - name: 00-bond0.netdev
      contents: |
        [NetDev]
        Name=bond0
        Kind=bond
        [Bond]
        Mode=802.3ad
        TransmitHashPolicy=layer3+4
        MIIMonitorSec=1s
        LACPTransmitRate=fast

    - name: 00-vbond0.netdev
      contents: |
        [NetDev]
        Name=vbond0
        Kind=macvlan
        [MACVLAN]
        Mode=bridge

    - name: 01-ethernet.network
      contents: |
        [Match]
        Name=en*
        [Network]
        Bond=bond0
        LLMNR=false

    - name: 02-ethernet.network
      contents: |
        [Match]
        Name=eth*
        [Network]
        Bond=bond0
        LLMNR=false

    - name: 02-bond0.network
      contents: |
        [Match]
        Name=bond0
        [Network]
        MACVLAN=vbond0
        LLMNR=false

    - name: 03-vbond0.network
      contents: |
        [Match]
        Name=vbond0
        [Network]
        Address={{.network.cidrv4}}
        Gateway={{.network.gateway}}
        LLMNR=false


{{ if index . "ssh_authorized_keys" }}
passwd:
  users:
    - name: core
      ssh_authorized_keys:
        {{ range $element := .ssh_authorized_keys }}
        - {{$element}}
        {{end}}
{{end}}
