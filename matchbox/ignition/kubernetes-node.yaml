---
systemd:
  units:
    - name: docker.service
      enable: true

    - name: oem-cloudinit.service
      enable: false
      mask: true

    - name: coreos-metadata-sshkeys@.service
      enable: false
      mask: true

    - name: locksmithd.service
      enable: false
      mask: true

    - name: containerd.service
      enable: true

    - name: update-engine.service
      enable: false
      mask: true

    - name: update-engine-stub.service
      enable: false
      mask: true

    - name: tpmd.service
      enable: false
      mask: true

    - name: tcsd.service
      enable: false
      mask: true

    - name: cni-gc.service
      enable: true
      contents: |
        [Unit]
        Before=network-online.target
        RefuseManualStart=true

        [Service]
        Type=oneshot
        RemainAfterExit=true
        ExecStartPre=-/bin/rm -Rfv /var/lib/cni
        ExecStart=/bin/mkdir -pv /var/lib/cni/networks
        Restart=no

        [Install]
        RequiredBy=multi-user.target

    - name: docker-gc.timer
      enable: true
      contents: |
        [Timer]
        OnBootSec=2min
        OnUnitActiveSec=5min
        [Install]
        WantedBy=timers.target

    - name: docker-gc.service
      enable: true
      contents: |
        [Unit]
        Requires=docker.service
        After=docker.service

        [Service]
        Type=oneshot
        ExecStart=/bin/bash -c '/usr/bin/docker rm $(/usr/bin/docker ps -aq)'
        Restart=no

        [Install]
        RequiredBy=multi-user.target

    - name: rkt-api.service
      enable: true
      contents: |
        [Service]
        LimitNOFILE=65826
        ExecStart=/usr/bin/rkt api-service
        Restart=always
        RestartSec=2

        [Install]
        RequiredBy=multi-user.target

    - name: rkt-metadata.service
      enable: true

    - name: enjoliver-agent.service
      enable: true
      contents: |
        [Service]
        ExecStart=/usr/bin/enjoliver-agent
        EnvironmentFile=/etc/metadata.env
        Restart=always
        RestartSec=30

        [Install]
        RequiredBy=multi-user.target

    - name: systemd-timesyncd.service
      enable: true

    - name: ntpd.service
      enable: false

    - name: etcd.service
      mask: true
      enable: false

    - name: etcd2.service
      mask: true
      enable: false

    - name: vault-token-kubelet@kubernetes.service
      enable: true
      contents: |
        [Service]
        ExecStartPre=/bin/mkdir -pv /etc/vault.d/%i
        ExecStart=/opt/bin/vault-token %i kubelet
        RestartSec={{.vault_polling_sec}}
        Restart=on-failure

        [Install]
        WantedBy=multi-user.target

    - name: vault-token-kube-apiserver@kubernetes.service
      enable: true
      contents: |
        [Service]
        ExecStartPre=/bin/mkdir -pv /etc/vault.d/%i
        ExecStart=/opt/bin/vault-token %i kube-apiserver
        RestartSec={{.vault_polling_sec}}
        Restart=on-failure

        [Install]
        WantedBy=multi-user.target

    - name: vault-token-peer@etcd-fleet.service
      enable: true
      contents: |
        [Service]
        ExecStartPre=/bin/mkdir -pv /etc/vault.d/%i
        ExecStart=/opt/bin/vault-token %i peer
        RestartSec={{.vault_polling_sec}}
        Restart=on-failure

        [Install]
        WantedBy=multi-user.target

    - name: vault-token-client@etcd-fleet.service
      enable: true
      contents: |
        [Service]
        ExecStartPre=/bin/mkdir -pv /etc/vault.d/%i
        ExecStart=/opt/bin/vault-token %i client
        RestartSec={{.vault_polling_sec}}
        Restart=on-failure

        [Install]
        WantedBy=multi-user.target

    - name: vault-pki-issue-kubelet@kubernetes.service
      enable: true
      contents: |
        [Service]
        EnvironmentFile=/etc/vault.d/%i/kubelet.token
        ExecStart=/opt/bin/vault-pki-issue %i kubelet
        RestartSec={{.vault_polling_sec}}
        Restart=on-failure

        [Install]
        WantedBy=multi-user.target

    - name: vault-pki-issue-kube-apiserver@kubernetes.service
      enable: true
      contents: |
        [Service]
        EnvironmentFile=/etc/vault.d/%i/kube-apiserver.token
        # Setting a common_name for RBAC generic name
        Environment=COMMON_NAME=node
        ExecStart=/opt/bin/vault-pki-issue %i kube-apiserver
        RestartSec={{.vault_polling_sec}}
        Restart=on-failure

        [Install]
        WantedBy=multi-user.target

    - name: vault-secret-service-accounts@kubernetes.service
      enable: true
      contents: |
        [Service]
        EnvironmentFile=/etc/vault.d/%i/kube-apiserver.token
        ExecStart=/opt/bin/vault-secret %i service-accounts
        RestartSec={{.vault_polling_sec}}
        Restart=on-failure

        [Install]
        WantedBy=multi-user.target

    - name: vault-pki-issue-peer@etcd-fleet.service
      enable: true
      contents: |
        [Service]
        EnvironmentFile=/etc/vault.d/%i/peer.token
        ExecStart=/opt/bin/vault-pki-issue %i peer
        RestartSec={{.vault_polling_sec}}
        Restart=on-failure

        [Install]
        WantedBy=multi-user.target

    - name: vault-pki-issue-client@etcd-fleet.service
      enable: true
      contents: |
        [Service]
        EnvironmentFile=/etc/vault.d/%i/client.token
        ExecStart=/opt/bin/vault-pki-issue %i client
        RestartSec={{.vault_polling_sec}}
        Restart=on-failure

        [Install]
        WantedBy=multi-user.target

    - name: etcd3@fleet.service
      enable: true
      contents: |
        [Unit]
        Conflicts=etcd.service etcd2.service

        [Service]
        Type=notify
        EnvironmentFile=/etc/etcd-%i.env
        ExecStartPre=/bin/ls -l \
          /etc/vault.d/etcd-%i/client.certificate \
          /etc/vault.d/etcd-%i/client.private_key \
          /etc/vault.d/etcd-%i/client.issuing_ca \
          /etc/vault.d/etcd-%i/peer.certificate \
          /etc/vault.d/etcd-%i/peer.private_key \
          /etc/vault.d/etcd-%i/peer.issuing_ca
        ExecStart=/usr/bin/etcd \
          --cert-file /etc/vault.d/etcd-%i/client.certificate \
          --key-file /etc/vault.d/etcd-%i/client.private_key \
          --trusted-ca-file /etc/vault.d/etcd-%i/client.issuing_ca \
          --client-cert-auth \
          \
          --peer-cert-file /etc/vault.d/etcd-%i/peer.certificate \
          --peer-key-file /etc/vault.d/etcd-%i/peer.private_key \
          --peer-trusted-ca-file /etc/vault.d/etcd-%i/peer.issuing_ca \
          --peer-client-cert-auth
        RestartSec=5s
        Restart=on-failure
        LimitNOFILE=65826

        [Install]
        WantedBy=multi-user.target

    - name: haproxy.service
      enable: true
      contents: |
        [Unit]
        Requires=network.target
        After=network.target

        [Service]
        ExecStart=/usr/sbin/haproxy -f /etc/haproxy/haproxy.cfg
        Restart=always
        RestartSec=5s

        [Install]
        WantedBy=multi-user.target

    - name: kubelet.service
      enable: true
      contents: |
        [Unit]
        After=rkt-api.service
        Requires=rkt-api.service
        After=haproxy.service

        [Service]
        Environment=PATH=/opt/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin
        ExecStartPre=/bin/ls -l \
          /etc/vault.d/kubernetes/kubelet.certificate \
          /etc/vault.d/kubernetes/kubelet.private_key \
          /etc/vault.d/kubernetes/kubelet.issuing_ca

        ExecStart=/usr/bin/hyperkube kubelet \
          --hairpin-mode=none \
          --non-masquerade-cidr={{.network.subnet}} \
          --network-plugin=cni \
          --cni-bin-dir=/usr/local/cni/bin/ \
          --cni-conf-dir=/etc/rkt/net.d \
          --hostname-override={{.kubernetes_node_name}} \
          --node-ip={{.kubernetes_node_ip}} \
          --register-schedulable=false \
          --allow-privileged=true \
          --enable-custom-metrics \
          --require-kubeconfig \
          --node-labels=node=true,diskProfile={{.disk_profile}} \
          --cloud-provider="" \
          --container-runtime=docker \
          --v=2 \
          --tls-cert-file=/etc/vault.d/kubernetes/kubelet.certificate \
          --tls-private-key-file=/etc/vault.d/kubernetes/kubelet.private_key \
          --client-ca-file=/etc/vault.d/kubernetes/kubelet.issuing_ca \
          --healthz-port {{.kubelet_healthz_port}}
        Restart=always
        RestartSec=5s

        [Install]
        WantedBy=multi-user.target

    - name: fleet.service
      enable: true
      dropins:
        - name: 10-cluster.conf
          contents: |
            [Unit]
            After=etcd3@fleet.service
            After=rkt-metadata.service

            [Service]
            EnvironmentFile=/etc/metadata.env
            ExecStartPre=/usr/bin/etcdctl --no-sync --endpoint http://127.0.0.1:{{.fleet_etcd_client_port}} ls
            Restart=always
            RestartSec=10

    - name: lifecycle-ready.service
      enable: true
      contents: |
        [Unit]
        After=kubelet.service

        [Service]
        Type=oneshot
        ExecStart=/opt/bin/lifecycle-ready
        RemainAfterExit=yes

        [Install]
        WantedBy=multi-user.target

    - name: lifecycle-update.service
      enable: true
      contents: |
        [Unit]
        After=etcd3@vault.service
        After=etcd3@fleet.service
        After=kubelet.service
        After=lifecycle-ready.service

        [Service]
        EnvironmentFile=/etc/metadata.env
        ExecStart=/opt/bin/lifecycle-update
        Restart=always
        RestartSec={{.lifecycle_update_polling_sec}}

        [Install]
        WantedBy=multi-user.target

    - name: rkt-gc.service
      enable: true
      dropins:
        - name: 10-rkt.conf
          contents: |
            [Service]
            # drop-in bugs if don't create an empty entry
            ExecStart=
            # rkt doesn't remove used images
            ExecStartPre=-/bin/bash -c '/usr/bin/rkt image list --no-legend | awk \'$8 ~ /(day|week|month)s?/ { system("/usr/bin/rkt image rm " $1) }\''
            # safety for the rkt lock
            ExecStartPre=-/bin/sleep 1
            ExecStartPre=-/usr/bin/rkt image gc --grace-period=12h
            # because the run-prepared by kubernetes can be gc
            ExecStart=/usr/bin/rkt gc --grace-period=3m
            [Install]
            RequiredBy=multi-user.target

    - name: rkt-gc.service
      enable: true
      contents: |
        [Service]
        ExecStartPre=-/bin/bash -c '/usr/bin/rkt image list --no-legend | awk \'$8 ~ /(day|week|month)s?/ { system("/usr/bin/rkt image rm " $1) }\''
        ExecStartPre=-/bin/sleep 1
        ExecStartPre=-/usr/bin/rkt image gc --grace-period=12h
        ExecStart=/usr/bin/rkt gc --grace-period=3m
        [Install]
        RequiredBy=multi-user.target

    - name: rkt-gc.timer
      enable: true
      contents: |
        [Timer]
        OnActiveSec=4min
        OnUnitActiveSec=10min
        [Install]
        WantedBy=timers.target

storage:
  files:
    - path: /etc/metadata.env
      filesystem: root
      mode: 0644
      contents:
        inline: |
          # Configuration created by matchbox wrapped with Enjoliver
          # Mainly by the ~/app/configs.yaml, ~/app/configs.py, ~/app/sync.py
          # Retrieve the current configuration by calling the following URL:
          # curl '{{.api_uri}}/metadata?{{.request.raw_query}}' | sort
          API_URI='{{.api_uri}}'
          CNI='{{.cni}}'
          DNS_ATTR_DC='{{.dns_attr.dc}}'
          DNS_ATTR_DOMAIN='{{.dns_attr.domain}}'
          DNS_ATTR_POS='{{.dns_attr.pos}}'
          DNS_ATTR_RACK='{{.dns_attr.rack}}'
          DNS_ATTR_SHORTNAME='{{.dns_attr.shortname}}'
          ETCD_NAME='{{.etcd_name}}'
          FLEET_ETCD_ADVERTISE_CLIENT_URLS='{{.fleet_etcd_advertise_client_urls}}'
          FLEET_ETCD_CLIENT_PORT='{{.fleet_etcd_client_port}}'
          FLEET_ETCD_DATA_DIR='{{.fleet_etcd_data_dir}}'
          FLEET_ETCD_INITIAL_ADVERTISE_PEER_URLS='{{.fleet_etcd_initial_advertise_peer_urls}}'
          FLEET_ETCD_INITIAL_CLUSTER='{{.fleet_etcd_initial_cluster}}'
          FLEET_ETCD_MEMBER_CLIENT_URI_LIST='{{.fleet_etcd_member_client_uri_list}}'
          HOSTNAME='{{.hostname}}'
          HYPERKUBE_IMAGE_URL='{{.hyperkube_image_url}}'
          KUBERNETES_APISERVER_INSECURE_PORT='{{.kubernetes_apiserver_insecure_port}}'
          KUBERNETES_ETCD_ADVERTISE_CLIENT_URLS='{{.kubernetes_etcd_advertise_client_urls}}'
          KUBERNETES_ETCD_CLIENT_PORT='{{.kubernetes_etcd_client_port}}'
          KUBERNETES_ETCD_DATA_DIR='{{.kubernetes_etcd_data_dir}}'
          KUBERNETES_ETCD_INITIAL_ADVERTISE_PEER_URLS='{{.kubernetes_etcd_initial_advertise_peer_urls}}'
          KUBERNETES_ETCD_INITIAL_CLUSTER='{{.kubernetes_etcd_initial_cluster}}'
          KUBERNETES_ETCD_MEMBER_CLIENT_URI_LIST='{{.kubernetes_etcd_member_client_uri_list}}'
          KUBERNETES_NODE_IP='{{.kubernetes_node_ip}}'
          KUBERNETES_NODE_NAME='{{.kubernetes_node_name}}'
          KUBERNETES_SERVICE_CLUSTER_IP_RANGE='{{.kubernetes_service_cluster_ip_range}}'
          KUBELET_HEALTHZ_PORT='{{.kubelet_healthz_port}}'
          MAC='{{.mac}}'
          NETWORK_CIDRV4='{{.network.cidrv4}}'
          NETWORK_GATEWAY='{{.network.gateway}}'
          NETWORK_IP='{{.network.ip_or_fqdn}}'
          REQUEST_QUERY_MAC='{{.request.query.mac}}'
          REQUEST_QUERY_UUID='{{.request.query.uuid}}'
          REQUEST_RAW_QUERY='{{.request.raw_query}}'
          ROLES='{{.roles}}'
          SELECTOR_MAC='{{.selector.mac}}'
          VAULT_PORT='{{.vault_port}}'

    - path: /etc/hostname
      filesystem: root
      mode: 0644
      contents:
        inline: |
          {{.hostname}}

    - path: /etc/rkt/paths.d/paths.json
      filesystem: root
      mode: 0644
      contents:
        inline: |
          {
            "rktKind": "paths",
            "rktVersion": "v1",
            "stage1-images": "/usr/lib/rkt/stage1-images"
          }

    - path: /etc/rkt/stage1.d/coreos.json
      filesystem: root
      mode: 0644
      contents:
        inline: |
          {
              "rktKind": "stage1",
              "rktVersion": "v1",
              "name": "coreos.com/rkt/stage1-coreos",
              "version": "v1.27.0",
              "location": "/usr/lib/rkt/stage1-images/stage1-coreos.aci"
          }

    - path: /etc/rkt/net.d/10-k8s.conf
      filesystem: root
      mode: 0644
      contents:
        inline: |
          {
            "name": "bond0",
            "type": "macvlan",
            "master": "bond0",
            "ipam": {{ .cni }}
          }

    - path: /etc/fleet/fleet.conf
      mode: 0644
      filesystem: root
      contents:
        inline: |
          etcd_servers = [ http://127.0.0.1:{{.fleet_etcd_client_port}},{{ .fleet_etcd_member_client_uri_list }} ]
          metadata = "name={{.dns_attr.shortname}},diskProfile={{.disk_profile}}"
          etcd_cafile=/etc/vault.d/etcd-fleet/client.issuing_ca
          etcd_certfile=/etc/vault.d/etcd-fleet/client.certificate
          etcd_keyfile=/etc/vault.d/etcd-fleet/client.private_key
          agent_ttl = 120s
          engine_reconcile_interval = 1
          etcd_request_timeout = 10
          disable_engine = true

    - path: /etc/modprobe.d/bonding.conf
      mode: 0644
      filesystem: root
      contents:
        inline: |
          options bonding mode=1 miimon=100

    - path: /etc/hosts
      mode: 0644
      filesystem: root
      contents:
        inline: |
          127.0.0.1	localhost
          {{ if index . "etc_hosts" }}
          {{ range $element := .etc_hosts }}
          {{$element}}
          {{end}}
          {{end}}

    - path: /etc/coreos/update.conf
      mode: 0644
      filesystem: root
      contents:
        inline: |
          GROUP=stable
          REBOOT_STRATEGY=off

    - path: /etc/systemd/timesyncd.conf
      mode: 0644
      filesystem: root
      contents:
        inline: |
          [Time]
          NTP={{.ntp}}
          FallbackNTP={{.fallbackntp}}

    - path: /etc/systemd/resolved.conf
      mode: 0644
      filesystem: root
      contents:
        inline: |
          [Resolve]
          DNS={{.nameservers}}
          LLMNR=false

    - path: /var/log/journal/.keep
      mode: 0644
      filesystem: root

    - path: /etc/etcd-fleet.env
      mode: 0644
      filesystem: root
      contents:
        inline: |
          ETCD_NAME={{.etcd_name}}
          ETCD_PROXY=on
          ETCD_INITIAL_CLUSTER={{.fleet_etcd_initial_cluster}}
          ETCD_ADVERTISE_CLIENT_URLS=https://{{.network.ip_or_fqdn}}:{{.fleet_etcd_client_port}}
          ETCD_MEMBER_CLIENT_URI_LIST={{.fleet_etcd_member_client_uri_list}}
          ETCD_DATA_DIR={{.fleet_etcd_data_dir}}
          ETCD_LISTEN_CLIENT_URLS="http://127.0.0.1:{{.fleet_etcd_client_port}},https://{{.network.ip_or_fqdn}}:{{.fleet_etcd_client_port}},http://{{.network.ip_or_fqdn}}:2379"

    - path: /opt/bin/etcdctl3
      filesystem: root
      mode: 0544
      contents:
        inline: |
          #!/bin/bash
          ETCDCTL_API=3 exec /usr/bin/etcdctl $@

    - path: /etc/profile.d/common.sh
      mode: 0755
      filesystem: root
      contents:
        inline: |
           export PATH=/opt/bin:$PATH
           export PRIVATE_IPV4={{.network.ip_or_fqdn}}

    - path: /etc/modules-load.d/network.conf
      mode: 0644
      filesystem: root
      contents:
        inline: |
          ip_tables
          iptable_nat
          nf_nat

    - path: /opt/bin/lifecycle-update
      mode: 0544
      filesystem: root
      contents:
        inline: |
          #!/usr/bin/env bash

          set -e
          set -o pipefail

          STATUS=$(curl -f -XPOST "{{.api_uri}}/lifecycle/ignition/{{.request.raw_query}}" \
                -d @/usr/share/oem/coreos-install.json \
                -H "Content-Type: application/json" \
                -w "%{http_code}" -o /dev/null)

          set +e
          if [[ ${STATUS} -ne 210 ]]
          then
              PRESET=/run/systemd-preset-all
              echo "Checking if preset on hold"
              grep -c "^Created symlink" ${PRESET}
              if [[ $? -ne 0 ]]
              then
                echo "Nothing to do: ${STATUS}, no preset in ${PRESET}"
                exit 0
              fi
              cat ${PRESET} || {
                echo "Nothing to do: ${STATUS}, no file preset ${PRESET}"
                exit 0
              }
          fi

          set -xe

          curl -f "{{.api_uri}}/ignition?{{.request.raw_query}}" -o /tmp/coreos-install.json
          cat /tmp/coreos-install.json | jq -e . > /dev/null

          STRATEGY=$(curl -f "{{.api_uri}}/lifecycle/rolling/{{.request.raw_query}}" | jq -re .strategy ) || {
            exit 0
          }

          echo "Update enable by strategy: ${STRATEGY}"

          echo "locksmithctl operations..."
          locksmithctl -endpoint http://127.0.0.1:{{.fleet_etcd_client_port}} status
          locksmithctl -endpoint http://127.0.0.1:{{.fleet_etcd_client_port}} unlock "{{.request.raw_query}}" || true
          locksmithctl -endpoint http://127.0.0.1:{{.fleet_etcd_client_port}} lock "{{.request.raw_query}}"

          echo "stopping fleet..."
          systemctl stop fleet.service

          echo "draining node..."
          kubectl --kubeconfig /var/lib/kubelet/kubeconfig \
            drain {{.kubernetes_node_name}} --force --ignore-daemonsets --delete-local-data || true

          set +e
          for service in kubelet haproxy etcd3@fleet
          do
            systemctl stop ${service}.service
          done

          set -e

          DEVICE=/dev/sda
          DISK_GUID="00000000-0000-0000-0000-000000000001"
          sgdisk --disk-guid=${DISK_GUID} ${DEVICE}
          cgpt show -v ${DEVICE} | grep -c ${DISK_GUID}

          if [[ ${STRATEGY} == "kexec" ]]
          then
            kexec --reuse-cmdline \
                  --append="coreos.first_boot=1 coreos.randomize_disk_guid=${DISK_GUID}" \
                  -l /usr/boot/vmlinuz
          fi

          cp -v /tmp/coreos-install.json /usr/share/oem/coreos-install.json

          systemctl ${STRATEGY}


    - path: /opt/bin/lifecycle-ready
      mode: 0544
      filesystem: root
      contents:
        inline: |
          #!/usr/bin/env bash
          export PATH=/usr/bin:/opt/bin/:${PATH}

          set -o pipefail

          function retry {
            until $@
            do
              echo "waiting for $@"
              sleep 1
            done
          }

          retry etcdctl \
             --cert-file /etc/vault.d/etcd-fleet/client.certificate \
             --key-file /etc/vault.d/etcd-fleet/client.private_key \
             --ca-file /etc/vault.d/etcd-fleet/client.issuing_ca \
             --endpoints http://127.0.0.1:{{.fleet_etcd_client_port}} cluster-health

          retry curl -fs http://127.0.0.1:10248/healthz

          retry fleetctl --endpoint http://127.0.0.1:{{.fleet_etcd_client_port}} \
            --driver etcd list-machines \
            --fields ip --no-legend | grep -c {{.network.ip}}

          retry fleetctl \
            --ca-file /etc/vault.d/etcd-fleet/client.issuing_ca \
            --cert-file /etc/vault.d/etcd-fleet/client.certificate \
            --key-file /etc/vault.d/etcd-fleet/client.private_key \
            --driver etcd --endpoint https://{{.network.ip_or_fqdn}}:{{.fleet_etcd_client_port}},{{.fleet_etcd_member_client_uri_list}} \
            list-machines --fields ip --no-legend | grep -c {{.network.ip}}

          retry kubectl --kubeconfig /var/lib/kubelet/kubeconfig uncordon {{.kubernetes_node_name}}

          systemctl preset-all 2>&1 | tee -a /run/systemd-preset-all

          set -e

          echo "remove the lock"
          locksmithctl -endpoint http://127.0.0.1:{{.fleet_etcd_client_port}} unlock "{{.request.raw_query}}" || true
          echo "$(hostname) {{.network.ip_or_fqdn}} is ready"

    - path: /opt/bin/vault-token
      mode: 0544
      filesystem: root
      contents:
        inline: |
          #!/usr/bin/env bash

          export PATH=/opt/bin:${PATH}
          set -o pipefail

          test ${1} || {
            echo 'missing $1 eg: etcd-kubernetes'
            exit 2
          }
          test ${2} || {
            echo 'missing $2 eg: peer'
            exit 2
          }

          while true
          do
            for ve in $(echo -n {{ .vault_etcd_member_client_uri_list }} | tr ',' '\n' | shuf)
            do
              REQ="${ve}/v2/keys/token/${1}/${2}"
              echo ${REQ}

              TOKEN=$(curl --insecure -fL ${REQ} | jq -re .node.value)
              if [[ $? -eq 0 ]]
              then
                echo "TOKEN=${TOKEN}" > /etc/vault.d/${1}/${2}.new

                cmp /etc/vault.d/${1}/${2}.token /etc/vault.d/${1}/${2}.new
                if [[ $? -eq 0 ]]
                then
                  echo "/etc/vault.d/${1}/${2}.token is up-to-date"
                  exit 0
                else
                  mv -v /etc/vault.d/${1}/${2}.new /etc/vault.d/${1}/${2}.token
                  exit 0
                fi
              fi
            done
            sleep 5
          done

    - path: /opt/bin/vault-fetch-ca
      mode: 0544
      filesystem: root
      contents:
        inline: |
          #!/usr/bin/env bash

          export PATH=/opt/bin:${PATH}
          export VAULT_CAPATH=/etc/vault.d/vault/server.issuing_ca
          set -o pipefail

          # TODO consul-template or check if renew
          test -s ${VAULT_CAPATH} || {
          for ve in $(echo -n {{ .vault_ip_list }} | tr ',' '\n' | shuf)
          do
              REQ=https://${ve}:{{.vault_etcd_client_port}}/v2/keys/pki/vault/server.issuing_ca
              echo "GET ${REQ}"
              curl -fs --insecure ${REQ} \
                | jq -re .node.value > ${VAULT_CAPATH} && {
                  ln -svf ${VAULT_CAPATH} /etc/ssl/certs/vault
                  ls -l ${VAULT_CAPATH} /etc/ssl/certs/vault
                  exit $?
              }
              sleep 5
          done
          }
          exit 2

    - path: /opt/bin/vault-pki-issue
      mode: 0544
      filesystem: root
      contents:
        inline: |
          #!/usr/bin/env bash

          set -x

          export PATH=/opt/bin:${PATH}
          export VAULT_CAPATH=/etc/vault.d/vault/server.issuing_ca
          set -o pipefail

          test ${1} || {
            echo 'missing $1 eg: etcd-kubernetes'
            exit 2
          }
          test ${2} || {
            echo 'missing $2 eg: peer'
            exit 2
          }
          test ${TOKEN} || {
            echo 'missing ${TOKEN}'
            exit 2
          }

          # TODO consul-template or check if renew
          test -s /etc/vault.d/${1}/${2}-pki.json && exit 0
          test -s ${VAULT_CAPATH} || {
            /opt/bin/vault-fetch-ca || exit $?
          }

          # The common_name default is the ip address
          test ${COMMON_NAME} || {
            COMMON_NAME={{.network.ip_or_fqdn}}
            echo "setting COMMON_NAME=${COMMON_NAME}"
          }

          while true
          do
            for ve in $(echo -n {{ .vault_ip_list }} | tr ',' '\n' | shuf)
            do
              REQ="https://${ve}:{{.vault_port}}/v1/pki/${1}/issue/${2}"
              echo "POST ${REQ}"
              curl ${REQ} \
                --cacert ${VAULT_CAPATH} -Lfs \
                --header "X-Vault-Token: ${TOKEN}" \
                -XPOST --data "{
                  \"common_name\": \"${COMMON_NAME}\",
                  \"ttl\": \"17520h\",
                  \"ip_sans\": \"{{.network.ip_or_fqdn}},127.0.0.1,{{.kubernetes_service_cluster_ip}}\"
                }" > /etc/vault.d/${1}/${2}-pki.json
              if [[ $? -eq 0 ]]
              then
                for item in certificate issuing_ca private_key
                do
                  jq -re .data.${item} /etc/vault.d/${1}/${2}-pki.json > /etc/vault.d/${1}/${2}.${item} || exit 2
                done
                exit 0
              fi
            done
            sleep 5
          done

    - path: /opt/bin/vault-secret
      mode: 0544
      filesystem: root
      contents:
        inline: |
          #!/usr/bin/env bash

          export PATH=/opt/bin:${PATH}
          export VAULT_CAPATH=/etc/vault.d/vault/server.issuing_ca
          set -o pipefail

          test ${1} || {
            echo 'missing $1 eg: kubernetes'
            exit 2
          }
          test ${2} || {
            echo 'missing $2 eg: service-accounts'
            exit 2
          }
          test ${TOKEN} || {
            echo 'missing ${TOKEN}'
            exit 2
          }

          # TODO consul-template or check if renew
          test -s /etc/vault.d/${1}/${2}-secret.json && exit 0
          test -s ${VAULT_CAPATH} || {
            /opt/bin/vault-fetch-ca || exit $?
          }

          while true
          do
            for ve in $(echo -n {{ .vault_ip_list }} | tr ',' '\n' | shuf)
            do
              REQ="https://${ve}:{{.vault_port}}/v1/secret/${1}/${2}"
              echo "GET ${REQ}"
              curl ${REQ} -Lfs \
                --cacert ${VAULT_CAPATH} \
                --header "X-Vault-Token: ${TOKEN}" > /etc/vault.d/${1}/${2}-secret.json
              if [[ $? -eq 0 ]]
              then
                for item in key
                do
                  jq -re .data.${item} /etc/vault.d/${1}/${2}-secret.json > /etc/vault.d/${1}/${2}.${item} || exit 2
                done
                exit 0
              fi
            done
            sleep 5
          done

    - path: /etc/vault.d/vault/.keep
      mode: 0600
      filesystem: root

    - path: /etc/profile.d/alias.sh
      mode: 0755
      filesystem: root
      contents:
        inline: |
          if [[ $- != *i* ]] ; then
                   return
          fi
          alias ls='ls --color=auto'
          alias la='ls -la'
          alias ll='ls -ll'

    - path: /opt/bin/rbd
      mode: 0555
      filesystem: root
      contents:
        inline: |
          #!/bin/bash
          # LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/rbd/lib /opt/rbd/bin/rbd
          mkdir -p /etc/ceph
          rkt --insecure-options=all run \
            --volume data,kind=host,source=/etc/ceph \
            --mount volume=data,target=/etc/ceph \
            --interactive {{.cephtools_image_url}} -- $@

    - path: /etc/profile.d/alias.sh
      mode: 0755
      filesystem: root
      contents:
        inline: |
          if [[ $- != *i* ]] ; then
                   return
          fi
          alias ls='ls --color=auto'
          alias la='ls -la'
          alias ll='ls -ll'

    - path: /etc/haproxy/haproxy.cfg
      mode: 0755
      filesystem: root
      contents:
        inline: |
          global
            stats timeout 30s
            stats socket /var/run/haproxy.sock mode 666 level admin

          defaults
            timeout client  100s
            timeout server  100s
            timeout connect 100s
            timeout queue   100s

          listen stats
            mode http
            bind {{.network.ip_or_fqdn}}:1937
            stats enable
            stats uri /

          frontend kube-secure
            mode    tcp
            option  tcplog
            bind    0.0.0.0:6443
            default_backend kubernetes-backend

          backend kubernetes-backend
            balance roundrobin
            mode tcp
            {{ range $i, $element := .etcd_member_kubernetes_control_plane_ip }}
            server kubernetes-{{$i}} {{$element}}:6443 check
            {{end}}

    - path: /etc/kubernetes/kube-proxy/kubeconfig
      mode: 0400
      filesystem: root
      contents:
        inline: |
          apiVersion: v1
          kind: Config
          clusters:
            - cluster:
                certificate-authority: /etc/secrets/kube-apiserver.issuing_ca
                server: https://127.0.0.1:6443
              name: kubernetes
          contexts:
            - context:
                cluster: kubernetes
                user: proxy
              name: proxy-to-kubernetes
          current-context: proxy-to-kubernetes
          users:
            - name: proxy
              user:
                client-certificate: /etc/secrets/kube-apiserver.certificate
                client-key: /etc/secrets/kube-apiserver.private_key

    - path: /var/lib/kubelet/kubeconfig
      mode: 0400
      filesystem: root
      contents:
        inline: |
          apiVersion: v1
          kind: Config
          clusters:
            - cluster:
                certificate-authority: /etc/vault.d/kubernetes/kube-apiserver.issuing_ca
                server: https://127.0.0.1:6443
              name: kubernetes
          contexts:
            - context:
                cluster: kubernetes
                user: kubelet
              name: kubelet-to-kubernetes
          current-context: kubelet-to-kubernetes
          users:
            - name: kubelet
              user:
                client-certificate: /etc/vault.d/kubernetes/kube-apiserver.certificate
                client-key: /etc/vault.d/kubernetes/kube-apiserver.private_key

networkd:
  units:
    - name: lo.network
      contents: |
        [Match]
        Name=lo
        [Network]
        Address={{.kubernetes_service_cluster_ip_range}}

    - name: 00-bond0.netdev
      contents: |
        [NetDev]
        Name=bond0
        Kind=bond
        [Bond]
        Mode=802.3ad
        TransmitHashPolicy=layer3+4
        MIIMonitorSec=1s
        LACPTransmitRate=fast

    - name: 00-vbond0.netdev
      contents: |
        [NetDev]
        Name=vbond0
        Kind=macvlan
        [MACVLAN]
        Mode=bridge

    - name: 01-ethernet.network
      contents: |
        [Match]
        Name=en*
        [Network]
        Bond=bond0
        LLMNR=false

    - name: 02-ethernet.network
      contents: |
        [Match]
        Name=eth*
        [Network]
        Bond=bond0
        LLMNR=false

    - name: 02-bond0.network
      contents: |
        [Match]
        Name=bond0
        [Network]
        MACVLAN=vbond0
        LLMNR=false

    - name: 03-vbond0.network
      contents: |
        [Match]
        Name=vbond0
        [Network]
        Address={{.network.cidrv4}}
        Gateway={{.network.gateway}}
        LLMNR=false


{{ if index . "ssh_authorized_keys" }}
passwd:
  users:
    - name: core
      ssh_authorized_keys:
        {{ range $element := .ssh_authorized_keys }}
        - {{$element}}
        {{end}}
{{end}}
